\documentclass[11pt]{exam}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage[spanish]{babel}
\usepackage{listings}
\usepackage{float}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{parcolumns}
\usepackage{array}

\newenvironment{conditions}
{\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}\:{}$} l}}
	{\end{tabular}\par\vspace{\belowdisplayskip}}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                                   
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\title{Práctica 2}
\author{Laura Rodríguez Navas \\ rodrigueznavas@posgrado.uimp.es}
\date{{\selectlanguage{spanish}\today} }

\pagestyle{plain}

\begin{document}
	
\maketitle

\renewcommand{\tablename}{Tabla}
\renewcommand{\lstlistingname}{Código}

\section{Introducción}\label{introduccion}

Esta práctica se apoya en una plataforma que recrea el videojuego clásico \href{1https://en.wikipedia.org/wiki/Pac-Man}{Pac-Man}. Utiliza una versión muy simplificada del juego para poder desarrollar un sistema de control automático y explorar algunas capacidades del Aprendizaje por Refuerzo aprendidas durante la asignatura. Especialmente, en esta práctica se aplica el algoritmo \textit{Q-learning} para construir un agente que funcione de forma automática en los tres mapas disponibles: \textit{lab1.lay, lab2.lay y lab3.lay}. El objetivo de este agente será maximizar la puntuación obtenida en una partida.

Asimismo, en este documento se describen las tareas realizadas, que se dividen en distintas fases (definición de los estados, función de refuerzo, construcción del agente y evaluación), y que se detallan a continuación.

En la sección~\ref{estados} se justifica el conjunto de atributos elegido y su rango para la definición de los estados. Una vez se ha elegido el conjunto de atributos que se van a utilizar para representar cada estado, en la sección~\ref{refuerzo} se detalla el diseño de la función de refuerzo que vamos a emplear y que permitirá al agente lograr su objetivo de maximizar la puntuación obtenida en una partida. Después de esto, en la sección~\ref{codigo} se explica cómo se ha procedido a la construcción del agente con el fin de funcionar bien en todos los laberintos. Cuando se obtiene el agente, en la sección~\ref{resultados} se presentan los resultados de las puntuaciones que ha obtenido en los mapas proporcionados y se añaden comentarios sobre su comportamiento. Finalmente, en la parte final del documento se añaden las conclusiones (sección~\ref{conclusiones}) y un apéndice que contiene métodos auxiliares del código desarrollado en las secciones~\ref{refuerzo} y \ref{codigo}.

\section{Definición de los estados}\label{estados}

En esta sección, a fin de definir los estados, hay que seleccionar unos valores adecuados para los parámetros: \textit{nRowsQTable, alpha, gamma y epsilon}. Primero intentaremos descubrir el valor del parámetro \textit{nRowsQTable}, o que es lo mismo, intentaremos descubrir cuántas filas debe tener la \textit{QTable}. Este valor dependerá de las características seleccionadas para representar los estados, ya que según estas características tendremos un número diferente de filas en la \textit{QTable}. Nos fijamos en la información que nos proporciona la función \textit{printInfo}, concretamente en la información de los mapas \textit{Walls} y \textit{Food}. Según esta información (T/F) y las direcciones que puede ejecutar el agente: north, east, south y west, las características elegidas son:

\begin{parcolumns}{2}
	\colplacechunks
	\colchunk{
		\begin{itemize}
			\item nearest\_ghost\_north, no\_wall
			\item nearest\_ghost\_south, no\_wall
			\item nearest\_ghost\_east, no\_wall
			\item nearest\_ghost\_west, no\_wall
			\item nearest\_ghost\_north, wall
			\item nearest\_ghost\_south, wall
			\item nearest\_ghost\_east, wall
			\item nearest\_ghost\_west, wall
		\end{itemize}}
	\colchunk{
		\begin{itemize}
			\item nearest\_ghost\_north, no\_food
			\item nearest\_ghost\_south, no\_food
			\item nearest\_ghost\_east, no\_food
			\item nearest\_ghost\_west, no\_food
			\item nearest\_ghost\_north, food
			\item nearest\_ghost\_south, food
			\item nearest\_ghost\_east, food
			\item nearest\_ghost\_west, food
		\end{itemize}}
\end{parcolumns}
\vspace*{3mm}

Se han elegido 16 características, de este modo el valor del parámetro \textit{nRowsQTable} tendrá que ser igual a 16. Adaptamos el valor de \textit{nRowsQTable}:

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
self.nRowsQTable = 16
\end{lstlisting}

También tenemos que adaptar los parámetros \textit{alpha, gamma y epsilon}. Pero primero realizamos una pequeña descripción de estos:

\begin{itemize}
	\item Alpha ($\alpha$). Este valor toma valores entre 0 y 1. Cuando este valor es 0, entonces no hay aprendizaje y siempre se utiliza el valor de $Q$ inicial. Cuando este valor es 1, entonces el aprendizaje no conserva ningún porcentaje del valor de $Q$. Es decir, si el valor $\alpha$ se acerca a 0, el valor de $Q$ no variará mucho; y si el valor $\alpha$ se acerca a 1, el valor de $Q$ variará mucho.
	\item Gamma ($\gamma$). Este valor toma valores entre 0 y 1. Cuando este valor es 0, entonces el aprendizaje ignorará el valor dado por el modelo de aprendizaje y aprenderá solo con el valor de la recompensa. Cuando este valor es 1, entonces el aprendizaje utilizará el valor total dado por el modelo de aprendizaje.
	\item Epsilon ($\epsilon$). Numera los pasos para disminuir el valor $\alpha$. El valor $\alpha$ se reducirá para cada 10 decisiones tomadas. Por ejemplo, si este valor es 0.01, se reduce 0.01 el valor de $\alpha$ por cada 10 decisiones tomadas.
\end{itemize}

El proceso para adaptar los parámetros se realiza con el entrenamiento de una inteligencia simple (sin usar el aprendizaje automático). Esta inteligencia simple se compone de un objetivo muy singular, el agente Pac-Man atacando a uno de los fantasmas. Con esa finalidad, ejecutamos el siguiente comando con diferentes valores de \textit{alpha, gamma y epsilon} y observamos el valor resultante \textit{Average Score} en cada ejecución (ver Figura~\ref{average_score}). Este valor nos indica el valor promedio máximo de las puntuaciones obtenidas durante las partidas, o lo que es lo mismo, el valor promedio máximo de $Q$ durante las partidas.

\begin{lstlisting}[language=bash, basicstyle=\footnotesize]
python busters.py -p RLAgent -k 1 -l lab1.lay -n 100
\end{lstlisting}

Por ejemplo, con los valores: \textit{alpha = 1} , \textit{gamma = 0.8} y \textit{epsilon = 0.05}, el valor de \textit{Average Score} es menor que con los valores: \textit{alpha = 0.2}, \textit{gamma = 0.8} y \textit{epsilon = 0.05}. Otro ejemplo, con los valores: \textit{alpha = 0.4}, \textit{gamma = 0.8} y \textit{epsilon = 0.05}, el valor de \textit{Average Score} también es menor que con los valores: \textit{alpha = 0.2}, \textit{gamma = 0.8} y \textit{epsilon = 0.05}. Así, consideramos que la mejor asignación de parámetros es con los valores: \textit{alpha = 0.2}, \textit{gamma = 0.8} y \textit{epsilon = 0.05}.

Durante el proceso para adaptar los parámetros, los valores de \textit{epsilon} siempre toman valores muy próximos a 0, esto refleja un comportamiento aprendido en la \href{https://poliformat.upv.es/portal/site/ESP_0_2835/tool/c07b745a-0cfd-44f0-a7a2-9bb22f80c3f7?panel=Main}{práctica 1} de la asignatura. En esta práctica aprendimos que si en una estrategia $\epsilon$-greedy el valor de $\epsilon$ es más pequeño, entonces la probabilidad de que el agente \textit{Q-learning} tome decisiones aleatorias será menor. Así, si la aleatoriedad es menor, menor será la inestabilidad del agente Pac-Man. Como a consecuencia, el promedio del valor máximo de $Q$ será mayor, que corresponde a una mejor puntuación en el videojuego.

\newpage 

Los mejores valores encontrados para los parámetros son los siguientes:
\vspace*{3mm}

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
self.alpha = 0.2
self.gamma = 0.7
self.epsilon = 0.01
\end{lstlisting}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{average_score}
	\caption{Ejemplo de ejecución que muestra el  \textit{Average Score} obtenido.}
	\label{average_score}
\end{figure}


\section{Función de refuerzo}\label{refuerzo}

Una vez que se han adaptado los parámetros del agente Pac-Man podemos diseñar la función de refuerzo. La función de refuerzo determinará las recompensas que obtenga el agente. Determinar las recompensas que obtiene el agente con cada acción es clave para su aprendizaje, pues marcará cómo de buenas serán unas acciones frente a otras y por lo tanto determinará qué tipo de comportamientos decidimos potenciar. La siguiente lista enumera las diferentes acciones con sus valores de recompensa que se han elegido:

\begin{itemize}
	\item \textbf{Ganar}: este estado se muestra cuando el agente Pac-Man gana la partida. Ganamos 1000 puntos de recompensa.
	
	\item \textbf{Comer}: este estado se muestra cuando el agente Pac-Man se come a un fantasma o a un punto. Ganamos 100 puntos de recompensa.
	
	\item \textbf{Lejos de un fantasma y lejos de una pared}: este estado se muestra cuando el agente Pac-Man está al menos a cinco celdas del fantasma más cercano y de la pared más cercana. En este caso, ganamos 1 punto de recompensa.
	
	\item \textbf{Lejos de un fantasma y cerca de una pared}: este estado se muestra cuando el agente Pac-Man está al menos a cinco celdas del fantasma más cercano y está a un máximo de cuatro celdas de la pared más cercana. En este caso, perdemos 1 punto de recompensa.
	
	\item \textbf{Cerca de un fantasma y lejos de una pared}: este estado se muestra cuando el agente Pac-Man está a un máximo de cuatro celdas del fantasma más cercano y está al menos a cinco celdas de la pared más cercana.  En este caso, ganamos 1 punto de recompensa.
	
	\item \textbf{Cerca de un fantasma y cerca de una pared}: este estado se muestra cuando el agente Pac-Man está a un máximo de cuatro celdas del fantasma más cercano y de la pared más cercana. En este caso, perdemos 1 punto de recompensa.
	
	\item \textbf{Cerca de una pared}: este estado se muestra cuando el agente Pac-Man está a un máximo de cuatro celdas de la pared más cercana. En este caso, perdemos 4 puntos de recompensa.
	
	\item \textbf{Lejos de una pared}: este estado se muestra cuando el agente Pac-Man está al menos a cinco celdas de la pared más cercana. En este caso, ganamos 1 punto de recompensa.
\end{itemize}

En la acción Comer, el agente Pac-Man recuerda la posición del fantasma más cercano y elige la dirección (north, east, south o west) que lo lleva hasta él por el camino más corto. Como se puede observar, la definición de las recompensas es determinista, aunque las acciones dependan de la posición de los fantasmas. Por facilidad, se han definido valores de recompensa concretos para cada acción en particular. La función de refuerzo desarrollada se muestra a continuación:

\begin{lstlisting}[caption={Función de refuerzo.}, label={reward}, language=python, basicstyle=\footnotesize]
def getReward(self, state, nextState):
	"""
	Return a reward value based on the information of state and nextState
	"""
	reward = 0
	
	if nextState.isWin():
		return 1000
	
	# distancias al fantasma mas cercano en el siguiente estado
	next_state_ghost_distances = self.getGhostDistances(nextState)
	# distancias al fantasma mas cercano en el estado actual
	actual_state_ghost_distances = self.getGhostDistances(nextState)
	
	# distancia minima al fantasma mas cercano en el siguiente estado
	min_distance_ghost_next_State = min(next_state_ghost_distances, 
		key=lambda t: t[1])[0]
	min_ghost_distance_next_state = nextState.data.ghostDistances[min_distance_ghost_next_State]
	# distancia al fantasma mas cercano en el estado actual
	min_distance_ghost_actual_State = min(actual_state_ghost_distances, 
		key=lambda t: t[1])[0]
	min_ghost_distances_actual_state = state.data.ghostDistances[min_distance_ghost_actual_State]
	
	# numero de fantasmas en el estado actual
	number_ghost_actual_state = len(list(filter(lambda d: d is not None, 
		state.data.ghostDistances)))
	# numero de fantasmas en el siguiente estado
	number_ghost_next_state = len(list(filter(lambda d: d is not None, 
		nextState.data.ghostDistances)))
	
	# distancia a la pared mas cercana en el estado actual
	actual_state_has_walls = self.directionIsBlocked(state,
		state.getGhostPositions()[min_distance_ghost_next_State])
	# distancia a la pared mas cercana en el siguiente estado
		next_state_has_walls = self.directionIsBlocked(nextState,
	nextState.getGhostPositions()[min_distance_ghost_next_State])
	
	# come fantasma o punto
	if number_ghost_next_state < number_ghost_actual_state:
		reward += 100
	
	# mas lejos de un fantasma y lejos de una pared, no come
	if min_ghost_distance_next_state < min_ghost_distances_actual_state and not actual_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += 1
	
	# mas lejos de un fantasma y cerca de una pared, no come
	elif min_ghost_distance_next_state < min_ghost_distances_actual_state and actual_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += -1
	
	# mas cerca de un fantasma y lejos de una pared, no come
	elif min_ghost_distance_next_state > min_ghost_distances_actual_state and not actual_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += 1
	
	# mas cerca de un fantasma y cerca de una pared, no come
	elif min_ghost_distance_next_state > min_ghost_distances_actual_state and actual_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += -1
	
	# cerca de una pared, no come
	if not actual_state_has_walls and next_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward -= 4
	# lejos de una pared, no come
	elif actual_state_has_walls and not next_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += 1
	
	return reward	
\end{lstlisting}

Los métodos auxiliares \textit{getGhostDistances} (ver Código~\ref{getGhostDistances}) y \textit{directionIsBlocked} (ver Código~\ref{directionIsBlocked}) se incluyen en la sección~\ref*{apendice_reward}.

\section{Código desarrollado}\label{codigo}

Una vez que hemos seleccionado los parámetros que vamos a utilizar para representar cada estado, y hemos desarrollado la función de refuerzo que vamos a emplear (ver Código~\ref{reward}), procedemos a la construcción del agente Pac-Man. La implementación del agente consiste en ir determinando el estado actual en que se encuentra. Dado este estado, se elige la acción de acuerdo con los valores $Q$. Una vez elegida la acción, se determina cuál es la dirección más apropiada para la acción dada. A la hora de elegir la siguiente acción, se actualiza el valor $Q$ para el estado y la acción anterior, dado el estado actual obtenido y la mejor acción que devuelve la fórmula expresada en la ecuación~\ref{formula}, que podemos observar en la página siguiente. Finalmente dado el estado actual, el proceso comenzará de nuevo eligiendo la siguiente acción. Este procedimiento se recoge en la función \textit{update} (ver Código~\ref{update}), que lleva a cabo el algoritmo \textit{Q-learning}. 

Como se ha visto ya en la \href{https://poliformat.upv.es/portal/site/ESP_0_2835/tool/c07b745a-0cfd-44f0-a7a2-9bb22f80c3f7?panel=Main}{práctica 1} de la asignatura, los valores de la tabla $Q$ únicamente son actualizados cuando se cambia de estado, es decir, si al tomar una acción, el agente se mantiene en el mismo estado, la tabla $Q$ no se actualiza, sino que la recompensa se acumula, de forma que se suman todas las recompensas obtenidas mientras se permanece en un estado, y es cuando se pasa al siguiente, el momento de modificar la tabla $Q$. 

\newpage

La fórmula para obtener el nuevo valor $Q$:

\begin{equation}\label{formula}
	Q_{(t+1)}(s_{t},a_{t}) = (1-\alpha) * Q_{t}(s_{t},a_{t}) + \alpha * (R(s_{t},a_{t})) + V_{t}(s_{t+1}) - Q_{t}(s_{t},a_{t})
\end{equation}

donde,

\begin{conditions}
	Q_{(t+1)}(s_{t},a_{t}) & es el nuevo valor $Q$ dado el estado anterior $s_{t}$ y la acción anterior $a_{t}$. \\
	\alpha & es la tasa de aprendizaje. \\
	R(s_{t},a_{t}) & es el valor de recompensa dado el estado anterior $s_{t}$ y la acción anterior $a_{t}$.	\\
	Q_{t}(s_{t},a_{t}) & es el antiguo valor de $Q$ dado el estado anterior $s_{t}$ y la acción anterior $a_{t}$.	\\
	V_{t}(s_{t+1}) & es el valor de aprendizaje dado el nuevo estado $s_{t+1}$.
\end{conditions}

La función \textit{update} que lleva a cabo el algoritmo \textit{Q-learning}:
\vspace*{3mm}

\begin{lstlisting}[caption={Función update.}, label={update}, language=python, basicstyle=\footnotesize]
def update(self, state, action, nextState, reward):
	"""
	The parent class calls this to observe a
	state = action => nextState and reward transition.
	You should do your Q-Value update here
	"""
	reward = reward + self.getReward(state, nextState)  # actualizar recompensa
	
	print "Started in state:"
	self.printInfo(state)
	print "Took action: ", action
	print "Ended in state:"
	self.printInfo(nextState)
	print "Got reward: ", reward
	print "---------------------------------"

	# buscar el estado actual en la memoria de estados
	state_position = self.computePosition(state)
	action_position = self.actions[action]  # elegir accion
	# actualizar la tabla Q con la accion elegida
	self.q_table[state_position][action_position] = (1 - self.alpha) * 
		self.q_table[state_position][action_position] + self.alpha * (reward + 
			self.gamma * self.getValue(nextState))

	if nextState.isWin():
	# If a terminal state is reached
		self.writeQtable()
\end{lstlisting}

En la función \textit{update}, primero actualizamos el valor de la recompensa. Esto se debe a que en la función \textit{getReward} (ver Código \ref{reward}), el valor de la variable \textit{reward} inicialmente siempre es 0. Después buscamos el estado actual en la memoria de estados con la función \textit{computePosition} (ver Código~\ref{computePosition}), método que se incluye en la sección~\ref{apendice_update}. Porqué con la finalidad de que el agente Pac-Man pueda aprender, necesita tener a mano todos los estados que ya haya visitado anteriormente, para poder detectar en cuál de ellos se encuentra a medida que va jugando y, en caso de encontrarse en una situación nueva, registrarla para utilizarla posteriormente. En la implementación de la función \textit{statesMemory}, método que se incluye en la sección~\ref{apendice_update}, estos estados se guardan en forma de lista a medida que se van descubriendo (ver Código~\ref{statesMemory}).

Como se vió cuando se hablaba de \textit{Q-learning}, el agente aprende modificando los valores
de la función Q (los valores que hay almacenados en la tabla Q), de forma que cuando se
realiza la acción action\_position desde el estado state\_position, es el valor Q(s,a) el que se ve modificado de acuerdo a
la ecuación~\ref{formula}.

Una vez modificado el agente, observamos su comportamiento en uno de los laberintos disponibles de la práctica, ejecutándolo con el siguiente comando:

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
python busters.py -p RLAgent -k 1 -l lab1.lay -n 100
\end{lstlisting}

Con ello, hemos ejecutado durante 100 partidas el agente que acabamos de crear en el laberinto \textit{lab1}, que únicamente tiene un fantasma. Lo volvemos a ejecutar, pero esta vez en el laberinto \textit{lab2} que tiene dos fantasmas, con el uso del siguiente comando:

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
python busters.py -p RLAgent -k 2 -l lab2.lay -n 100
\end{lstlisting}

Finalmente, también lo ejecutamos en el último de los laberintos disponibles de la práctica, el laberinto \textit{lab3}, que tiene 3 fantasmas, pared en el interior del laberinto y un punto.

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
python busters.py -p RLAgent -k 3 -l lab3.lay -n 100
\end{lstlisting}

\section{Resultados}\label{resultados}

En esta sección exponemos los resultados obtenidos en las ejecuciones realizadas en la sección anterior (ver sección~\ref{codigo}), además sacamos algunas conclusiones con respecto a los resultados. Los resultados obtenidos son:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|} 
		\hline
		Mapa & Average Score \\ 
		\hline
		1 & 191.89 \\ 
		2 & 390.96 \\
		3 & 578 \\
		\hline
	\end{tabular}
	\caption{Resultados de las ejecuciones del agente Pac-Man.}
	\label{resultados_ejecuciones}
\end{table}
\section{Conclusiones}\label{conclusiones}


Todo el contenido de esta práctica se puede encontrar en el repositorio personal de GitHub: \url{https://github.com/lrodrin/masterAI/tree/master/A21/softpractica2}.

\section{Apéndice}\label{apendice}

\subsection{Métodos de la función \textit{get\_reward}}\label{apendice_reward}

\begin{lstlisting}[caption={Función getGhostDistances.}, label={getGhostDistances}, language=python, basicstyle=\footnotesize]
def getGhostDistances(gameState):
	"""
	Return distances to each of the ghosts on the map.
	"""
	return [(i, distance) for i, (distance, alive) in enumerate(zip(
		gameState.data.ghostDistances, gameState.getLivingGhosts()[1:])) if alive]
\end{lstlisting}

\begin{lstlisting}[caption={Función directionIsBlocked.}, label={directionIsBlocked}, language=python, basicstyle=\footnotesize]
def directionIsBlocked(gameState, ghost_position):
	"""
	Return True if directions are blocked (walls) and False otherwise (no walls).
	It also returns distances to blocking elements and non-blocking elements.
	"""
	walls = gameState.getWalls()
	walls_array = np.array(walls.data)
	pacman_position = gameState.getPacmanPosition()
	
	x_min = min(pacman_position[0], ghost_position[0])
	x_max = max(pacman_position[0], ghost_position[0]) + 1
	y_min = min(pacman_position[1], ghost_position[1])
	y_max = max(pacman_position[1], ghost_position[1]) + 1
	if y_min < 3:
		y_min = 3
	
	grid_beetween = walls_array[x_min:x_max, y_min:y_max]
	if len(grid_beetween) == 0:
		return False
	
	return np.any(np.all(grid_beetween, axis=1)) or np.any(np.all(grid_beetween, axis=0))
\end{lstlisting}

\subsection{Métodos de la función \textit{update}}\label{apendice_update}

\begin{lstlisting}[caption={Función computePosition.}, label={computePosition}, language=python, basicstyle=\footnotesize]
def computePosition(self, state):
	"""
	Compute the row of the qtable for a given state.
	"""
	pacman_ghost_direction, ghost_position = self.statesMemory(state)
	hasWall = self.directionIsBlocked(state, ghost_position)
	actions_value = 0
	for i, direction in enumerate(pacman_ghost_direction):
		actions_value += min(self.actions[direction], 2) + i * 4
	return int(hasWall) * 8 + actions_value
\end{lstlisting}

\begin{lstlisting}[caption={Función statesMemory.}, label={statesMemory}, language=python, basicstyle=\footnotesize]
def statesMemory(self, gameState):
	"""
	Create states memory.
	"""
	# posicion de pacman
	pacman_position = gameState.getPacmanPosition()
	
	# distancia minima al fantasma mas cercano
	living_ghosts_distances = self.getGhostDistances(gameState)
	min_distance_ghost_index = min(living_ghosts_distances, key=lambda t: t[1])[0]
	
	# posicion del fantasma mas cercano
	nearest_ghost_position = gameState.getGhostPositions()[min_distance_ghost_index]
	
	pacman_ghost_direction = []
	
	if (pacman_position[1] - nearest_ghost_position[1]) != 0 \
			and (pacman_position[1] - nearest_ghost_position[1]) > 0:
		pacman_ghost_direction.append("South")
	if (pacman_position[1] - nearest_ghost_position[1]) != 0 \
			and (pacman_position[1] - nearest_ghost_position[1]) < 0:
		pacman_ghost_direction.append("North")
	if (pacman_position[0] - nearest_ghost_position[0]) != 0 \
			and (pacman_position[0] - nearest_ghost_position[0]) > 0:
		pacman_ghost_direction.append("West")
	if (pacman_position[0] - nearest_ghost_position[0]) != 0 \
			and (pacman_position[0] - nearest_ghost_position[0]) < 0:
		pacman_ghost_direction.append("East")
	
	return pacman_ghost_direction, nearest_ghost_position
\end{lstlisting}

\end{document}
