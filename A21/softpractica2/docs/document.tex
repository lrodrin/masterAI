\documentclass[11pt]{exam}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage[spanish]{babel}
\usepackage{listings}
\usepackage{float}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{parcolumns}
\usepackage{array}

\newenvironment{conditions}
{\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}\:{}$} l}}
	{\end{tabular}\par\vspace{\belowdisplayskip}}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                                   
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\title{Práctica 2}
\author{Laura Rodríguez Navas \\ rodrigueznavas@posgrado.uimp.es}
\date{{\selectlanguage{spanish}\today} }

\pagestyle{plain}

\begin{document}
	
\maketitle

\section{Introducción}\label{introduccion}

Esta práctica se apoya en una plataforma que recrea el videojuego clásico \href{1https://en.wikipedia.org/wiki/Pac-Man}{Pac-Man}. Utiliza una versión muy simplificada del juego para poder desarrollar un sistema de control automático y explorar algunas capacidades del Aprendizaje por Refuerzo aprendidas durante la asignatura. Especialmente, en esta práctica se aplica el algoritmo \textit{Q-learning} para construir un agente que funcione de forma automática en los tres mapas disponibles: \textit{lab1.lay, lab2.lay y lab3.lay}. El objetivo de este agente será maximizar la puntuación obtenida en una partida.

Asimismo, en este documento se describen las tareas realizadas, que se dividen en distintas fases (definición de los estados, función de refuerzo, construcción del agente y evaluación), y que se detallan a continuación.

En la sección~\ref{estados} se justifica el conjunto de atributos elegido y su rango para la definición de los estados. Una vez se ha elegido el conjunto de atributos que se van a utilizar para representar cada estado, en la sección~\ref{refuerzo} se detalla el diseño de la función de refuerzo que vamos a emplear y que permitirá al agente lograr su objetivo de maximizar la puntuación obtenida en una partida. Después de esto, en la sección~\ref{codigo} se explica cómo se ha procedido a la construcción del agente con el fin de funcionar bien en todos los laberintos. Cuando se obtiene el agente, en la sección~\ref{resultados} se presentan los resultados de las puntuaciones que ha obtenido en los mapas proporcionados y se añaden comentarios sobre su comportamiento. Finalmente, en la parte final del documento se añaden las conclusiones (sección~\ref{conclusiones}) y un apéndice que contiene métodos auxiliares del código desarrollado en las secciones~\ref{refuerzo} y \ref{codigo}.

\section{Definición de los estados}\label{estados}

En esta sección, a fin de definir los estados, hay que seleccionar unos valores adecuados para los parámetros: \textit{self.nRowsQTable, self.alpha, self.gamma y self.epsilon}. Primero intentaremos descubrir el valor del parámetro \textit{self.nRowsQTable}, o que es lo mismo, intentaremos descubrir cuántas filas debe tener la \textit{QTable}. Este valor dependerá de las características seleccionadas para representar los estados, ya que según estas características tendremos un número diferente de filas en la \textit{QTable}. Nos fijamos en la información que nos proporciona la función \textit{printInfo}, concretamente en la información de los mapas \textit{Walls} y \textit{Food}. Según esta información (T/F) y las direcciones que puede ejecutar el agente: north, east, south y west, las características elegidas son:

\begin{parcolumns}{2}
	\colplacechunks
	\colchunk{
		\begin{itemize}
			\item nearest\_ghost\_north, no\_wall
			\item nearest\_ghost\_south, no\_wall
			\item nearest\_ghost\_east, no\_wall
			\item nearest\_ghost\_west, no\_wall
			\item nearest\_ghost\_north, wall
			\item nearest\_ghost\_south, wall
			\item nearest\_ghost\_east, wall
			\item nearest\_ghost\_west, wall
		\end{itemize}}
	\colchunk{
		\begin{itemize}
			\item nearest\_ghost\_north, no\_food
			\item nearest\_ghost\_south, no\_food
			\item nearest\_ghost\_east, no\_food
			\item nearest\_ghost\_west, no\_food
			\item nearest\_ghost\_north, food
			\item nearest\_ghost\_south, food
			\item nearest\_ghost\_east, food
			\item nearest\_ghost\_west, food
		\end{itemize}}
\end{parcolumns}
\vspace*{3mm}

Se han elegido 16 características, de este modo el valor del parámetro \textit{self.nRowsQTable} tendrá que ser igual a 16. Adaptamos el valor de \textit{self.nRowsQTable}:

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
self.nRowsQTable = 16
\end{lstlisting}

También tenemos que adaptar los parámetros \textit{self.alpha, self.gamma y self.epsilon}. Pero primero realizamos una pequeña descripción de estos:

\begin{itemize}
	\item Alpha ($\alpha$). Este valor toma valores entre 0 y 1. Cuando este valor es 0, entonces no hay aprendizaje y siempre se utiliza el valor de $Q$ inicial. Cuando este valor es 1, entonces el aprendizaje no conserva ningún porcentaje del valor de $Q$. Es decir, si el valor $\alpha$ se acerca a 0, el valor de $Q$ no variará mucho; y si el valor $\alpha$ se acerca a 1, el valor de $Q$ variará mucho.
	\item Gamma ($\gamma$). Este valor toma valores entre 0 y 1. Cuando este valor es 0, entonces el aprendizaje ignorará el valor dado por el modelo de aprendizaje y aprenderá solo con el valor de la recompensa. Cuando este valor es 1, entonces el aprendizaje utilizará el valor total dado por el modelo de aprendizaje.
	\item Epsilon ($\epsilon$). Numera los pasos para disminuir el valor $\alpha$. El valor $\alpha$ se reducirá para cada 10 decisiones tomadas. Por ejemplo, si este valor es 0.01, se reduce 0.01 el valor de $\alpha$ por cada 10 decisiones tomadas.
\end{itemize}

El proceso para adaptar los parámetros se realiza con el entrenamiento de una inteligencia simple (sin usar el aprendizaje automático). Esta inteligencia simple se compone de un objetivo muy singular, el agente Pac-Man atacando a uno de los fantasmas. Con esa finalidad, ejecutamos el siguiente comando con diferentes valores de \textit{self.alpha, self.gamma y self.epsilon} y observamos el valor resultante \textit{Average Score} en cada ejecución (ver Figura~\ref{average_score}). Este valor nos indica el valor promedio máximo de las puntuaciones obtenidas durante las partidas, o lo que es lo mismo, el valor promedio máximo de $Q$ durante las partidas.

\begin{lstlisting}[language=bash, basicstyle=\footnotesize]
python busters.py -p RLAgent -k 1 -l lab1.lay -n 100
\end{lstlisting}

Por ejemplo, con los valores: \textit{alpha = 1} , \textit{gamma = 0.8} y \textit{epsilon = 0.05}, el valor de \textit{Average Score} es menor que con los valores: \textit{self.alpha = 0.2}, \textit{self.gamma = 0.8} y \textit{self.epsilon = 0.05}. Otro ejemplo, con los valores: \textit{self.alpha = 0.4}, \textit{self.gamma = 0.8} y \textit{self.epsilon = 0.05}, el valor de \textit{Average Score} también es menor que con los valores: \textit{self.alpha = 0.2}, \textit{self.gamma = 0.8} y \textit{self.epsilon = 0.05}. Así, consideramos que la mejor asignación de parámetros es con los valores: \textit{self.alpha = 0.2}, \textit{self.gamma = 0.8} y \textit{self.epsilon = 0.05}.

Durante el proceso para adaptar los parámetros, los valores de \textit{epsilon} siempre toman valores muy próximos a 0, esto refleja un comportamiento aprendido en la \href{https://poliformat.upv.es/portal/site/ESP_0_2835/tool/c07b745a-0cfd-44f0-a7a2-9bb22f80c3f7?panel=Main}{práctica 1} de la asignatura. En esta práctica aprendimos que si en una estrategia $\epsilon$-greedy el valor de $\epsilon$ es más pequeño, entonces la probabilidad de que el agente \textit{Q-learning} tome decisiones aleatorias será menor. Así, si la aleatoriedad es menor, menor será la inestabilidad del agente Pac-Man. Como a consecuencia, el promedio del valor máximo de $Q$ será mayor, que corresponde a una mejor puntuación en el videojuego.

Los mejores valores encontrados para los parámetros son los siguientes:
\vspace*{3mm}

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
self.alpha = 0.2
self.gamma = 0.7
self.epsilon = 0.01
\end{lstlisting}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/average_score}
	\caption{Ejemplo de ejecución que muestra el  \textit{Average Score} obtenido.}
	\label{average_score}
\end{figure}


\section{Función de refuerzo}\label{refuerzo}

Una vez que se han adaptado los parámetros del agente Pac-Man podemos diseñar la función de refuerzo. La función de refuerzo determinará las recompensas que obtenga el agente. Determinar las recompensas que obtiene el agente con cada acción es clave para su aprendizaje, pues marcará cómo de buenas serán unas acciones frente a otras y por lo tanto determinará qué tipo de comportamientos decidimos potenciar. La siguiente lista enumera las diferentes acciones con sus valores de recompensa que se han elegido:

\begin{itemize}
	\item \textbf{Ganar}: este estado se muestra cuando el agente Pac-Man gana la partida. Ganamos 1000 puntos de recompensa.
	
	\item \textbf{Comer}: este estado se muestra cuando el agente Pac-Man se come a un fantasma o a un punto. Ganamos 100 puntos de recompensa.
	
	\item \textbf{Lejos de un fantasma y lejos de una pared}: este estado se muestra cuando el agente Pac-Man está al menos a cinco celdas del fantasma más cercano y de la pared más cercana. En este caso, ganamos 1 punto de recompensa.
	
	\item \textbf{Lejos de un fantasma y cerca de una pared}: este estado se muestra cuando el agente Pac-Man está al menos a cinco celdas del fantasma más cercano y está a un máximo de cuatro celdas de la pared más cercana. En este caso, perdemos 1 punto de recompensa.
	
	\item \textbf{Cerca de un fantasma y lejos de una pared}: este estado se muestra cuando el agente Pac-Man está a un máximo de cuatro celdas del fantasma más cercano y está al menos a cinco celdas de la pared más cercana.  En este caso, ganamos 3 puntos de recompensa.
	
	\item \textbf{Cerca de un fantasma y cerca de una pared}: este estado se muestra cuando el agente Pac-Man está a un máximo de cuatro celdas del fantasma más cercano y de la pared más cercana. En este caso, perdemos 1 punto de recompensa.
	
	\item \textbf{Cerca de una pared}: este estado se muestra cuando el agente Pac-Man está a un máximo de cuatro celdas de la pared más cercana. En este caso, perdemos 4 puntos de recompensa.
	
	\item \textbf{Lejos de una pared}: este estado se muestra cuando el agente Pac-Man está al menos a cinco celdas de la pared más cercana. En este caso, ganamos 1 punto de recompensa.
\end{itemize}

En la acción Comer, el agente Pac-Man recuerda la posición del fantasma más cercano y elige la dirección (north, east, south o west) que lo lleva hasta él por el camino más corto. Como se puede observar, la definición de las recompensas es determinista, aunque las acciones dependan de la posición de los fantasmas. Por facilidad, se han definido valores de recompensa concretos para cada acción en particular. La función de refuerzo desarrollada se muestra a continuación:

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
def getReward(self, state, nextState):
	"""
	Return a reward value based on the information of state and nextState
	"""
	reward = 0
	
	if nextState.isWin():
		return 1000
	
	# distancias al fantasma mas cercano en el siguiente estado
	next_state_ghost_distances = self.getGhostDistances(nextState)
	# distancias al fantasma mas cercano en el estado actual
	actual_state_ghost_distances = self.getGhostDistances(nextState)
	
	# distancia minima al fantasma mas cercano en el siguiente estado
	min_distance_ghost_next_State = min(next_state_ghost_distances, 
		key=lambda t: t[1])[0]
	min_ghost_distance_next_state = nextState.data.ghostDistances[min_distance_ghost_next_State]
	# distancia al fantasma mas cercano en el estado actual
	min_distance_ghost_actual_State = min(actual_state_ghost_distances, 
		key=lambda t: t[1])[0]
	min_ghost_distances_actual_state = state.data.ghostDistances[min_distance_ghost_actual_State]
	
	# numero de fantasmas en el estado actual
	number_ghost_actual_state = len(list(filter(lambda d: d is not None, 
		state.data.ghostDistances)))
	# numero de fantasmas en el siguiente estado
	number_ghost_next_state = len(list(filter(lambda d: d is not None, 
		nextState.data.ghostDistances)))
	
	# distancia a la pared mas cercana en el estado actual
	actual_state_has_walls = self.directionIsBlocked(state,
		state.getGhostPositions()[min_distance_ghost_next_State])
	# distancia a la pared mas cercana en el siguiente estado
		next_state_has_walls = self.directionIsBlocked(nextState,
	nextState.getGhostPositions()[min_distance_ghost_next_State])
	
	# come fantasma o punto
	if number_ghost_next_state < number_ghost_actual_state:
		reward += 100
	
	# mas lejos de un fantasma y lejos de una pared, no come
	if min_ghost_distance_next_state < min_ghost_distances_actual_state and not actual_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += 1
	
	# mas lejos de un fantasma y cerca de una pared, no come
	elif min_ghost_distance_next_state < min_ghost_distances_actual_state and actual_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += -1
	
	# mas cerca de un fantasma y lejos de una pared, no come
	elif min_ghost_distance_next_state > min_ghost_distances_actual_state and not actual_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += 3
	
	# mas cerca de un fantasma y cerca de una pared, no come
	elif min_ghost_distance_next_state > min_ghost_distances_actual_state and actual_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += -1
	
	# cerca de una pared, no come
	if not actual_state_has_walls and next_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward -= 4
	# lejos de una pared, no come
	elif actual_state_has_walls and not next_state_has_walls \
			and number_ghost_next_state == number_ghost_actual_state:
		reward += 1
	
	return reward	
\end{lstlisting}

Los métodos auxiliares \textit{self.getGhostDistance} y \textit{self.directionIsBlocked} se incluyen en la sección~\ref*{apendice_reward}.


\section{Código desarrollado}\label{codigo}

Una vez que hemos seleccionado los parámetros que vamos a utilizar para representar cada estado, y hemos desarrollado la función de refuerzo que vamos a emplear, procedemos a la construcción del agente Pac-Man. La implementación del agente consiste en ir determinando el estado actual en que se encuentra. Dado este estado, se elige la acción de acuerdo con los valores $Q$. Una vez elegida la acción, se determina cuál es la dirección más apropiada para la acción dada. Para elegir la siguiente acción, se actualiza el valor $Q$ para el estado y la acción anterior, dado el estado actual obtenido y la mejor acción que devuelve la fórmula expresada en~\ref{formula}. Finalmente dado el estado actual, el proceso comenzará de nuevo eligiendo la siguiente acción.

\begin{equation}\label{formula}
	Q_{(t+1)}(s_{t},a_{t}) = (1-\alpha) * Q_{t}(s_{t},a_{t}) + \alpha * (R(s_{t},a_{t})) + V_{t}(s_{t+1}) - Q_{t}(s_{t},a_{t})
\end{equation}

donde,

\begin{conditions}
Q_{(t+1)}(s_{t},a_{t}) & es el nuevo valor $Q$ dado el estado anterior $s_{t}$ y la acción anterior $a_{t}$. \\
\alpha & es la tasa de aprendizaje. \\
R(s_{t},a_{t}) & es el valor de recompensa dado el estado anterior $s_{t}$ y la acción anterior $a_{t}$.	\\
Q_{t}(s_{t},a_{t}) & es el antiguo valor de $Q$ dado el estado anterior $s_{t}$ y la acción anterior $a_{t}$.	\\
V_{t}(s_{t+1}) & es el valor de aprendizaje dado el nuevo estado $s_{t+1}$.
\end{conditions}

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
TODO
\end{lstlisting}


Este proyecto se divide en varias tareas pequeñas que deben completarse en consecuencia. La primera tarea gira en torno al diseño de Value-Iteration Agent. Este agente puede planificar su acción dado el conocimiento del entorno antes incluso de interactuar con él. Para ello, debemos calcular los valores Q de las acciones que seguirá el agente y elegir la mejor acción que devolverá el valor Q máximo. Después de diseñar el agente de iteración de valor, necesitamos cambiar el valor de descuento, ruido y vida.
recompensa para lograr la mejor política óptima.

La siguiente tarea es escribir código para Q learning agent. Este agente, a diferencia del agente de iteración de valor, necesita interactuar activamente con el entorno para que pueda aprender a través de las experiencias. Necesitamos aproximar la función Q de los pares estado-acción a partir de las muestras de Q (s, a) que observamos durante la interacción con el medio ambiente. Después de una aproximación exitosa, el agente debería poder tomar la mejor acción para lograr el objetivo. Q-learning Pacman debería poder ganar al menos el 80\% del tiempo.

Sin embargo, a medida que intentamos utilizar este agente en diseños más grandes, por ejemplo, el mundo Grid medio
El entrenamiento de PacMan de alguna manera ya no funcionará bien. Para resolver este problema, debemos
optimizar nuestro agente para implementar también un agente Q-learning aproximado que aprenda los pesos de
características de los estados, donde muchos estados pueden compartir las mismas características.



Una vez modificado el agente, observamos su comportamiento en uno de los laberintos disponibles de la práctica, ejecutándolo con el siguiente comando:

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
python busters.py -p RLAgent -k 1 -l lab1.lay -n 100
\end{lstlisting}

Con ello, hemos ejecutado durante 100 partidas el agente que acabamos de crear en el laberinto \textit{lab1}, que únicamente tiene un fantasma. Lo volvemos a ejecutar, pero esta vez en el laberinto \textit{lab2} que tiene dos fantasmas, con el uso del siguiente comando:

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
python busters.py -p RLAgent -k 2 -l lab2.lay -n 100
\end{lstlisting}

Finalmente, también lo ejecutamos en el último de los laberintos disponibles de la práctica, el laberinto \textit{lab3}, que tiene 3 fantasmas, pared en el interior del laberinto y un punto.

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
python busters.py -p RLAgent -k 3 -l lab3.lay -n 100
\end{lstlisting}

\section{Resultados}\label{resultados}

En esta sección exponemos los resultados obtenidos en las ejecuciones realizadas en la sección anterior (ver sección~\ref{codigo}), además sacamos algunas conclusiones con respecto a los resultados. Los resultados obtenidos son:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|} 
		\hline
		Mapa & Average Score \\ 
		\hline
		1 & 191.89 \\ 
		2 & 390.96 \\
		3 & 578 \\
		\hline
	\end{tabular}
	\caption{Resultados de las ejecuciones del agente Pac-Man.}
	\label{resultados_ejecuciones}
\end{table}
\section{Conclusiones}\label{conclusiones}


Todo el contenido de esta práctica se puede encontrar en el repositorio personal de GitHub: \url{https://github.com/lrodrin/masterAI/tree/master/A21/softpractica2}.

\section{Apéndice}\label{apendice}

\subsection{Métodos de la función \textit{get\_reward}}\label{apendice_reward}

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
def getGhostDistances(gameState):
	"""
	Return distances to each of the ghosts on the map.
	"""
	return [(i, distance) for i, (distance, alive) in enumerate(zip(
		gameState.data.ghostDistances, gameState.getLivingGhosts()[1:])) if alive]
\end{lstlisting}

\begin{lstlisting}[language=python, basicstyle=\footnotesize]
def directionIsBlocked(gameState, ghost_position):
	"""
	Return True if directions are blocked (walls) and False otherwise (no walls).
	It also returns distances to blocking elements and non-blocking elements.
	"""
	walls = gameState.getWalls()
	walls_array = np.array(walls.data)
	pacman_position = gameState.getPacmanPosition()
	
	x_min = min(pacman_position[0], ghost_position[0])
	x_max = max(pacman_position[0], ghost_position[0]) + 1
	y_min = min(pacman_position[1], ghost_position[1])
	y_max = max(pacman_position[1], ghost_position[1]) + 1
	if y_min < 3:
		y_min = 3
	
	grid_beetween = walls_array[x_min:x_max, y_min:y_max]
	if len(grid_beetween) == 0:
		return False
	
	return np.any(np.all(grid_beetween, axis=1)) or np.any(np.all(grid_beetween, axis=0))
\end{lstlisting}

\subsection{Métodos de la función \textit{update}}\label{apendice_update}


\end{document}
