---
title: "Práctica: clustering en R"
author: "Laura Rodríguez Navas"
date: "12/07/2020"
output:
  pdf_document: 
    fig_caption: yes
    keep_tex: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Parte introductoria de la práctica

Para la correcta ejecución de esta práctica, el estudiante debe verificar que los paquetes **mlbench** y **caret** están correctamente instalados en la plataforma R. Para instalar estos paquetes en R, solo debe ejecutar:

- **install.packages("mlbench")**
- **install.packages("caret")**

Además, para esta práctica se ha decidido que se necesitan los siguientes paquetes de R:

- **install.packages("factoextra")**
- **install.packages("ggdendro")**
- **install.packages("corrplot")**

Luego de haber instalado los paquetes en R, pasamos a cargar en memoria las librerías que usaremos.

```{r message=FALSE, warning=FALSE}
library(mlbench)
library(caret)
library(factoextra)
library(ggdendro)
library(corrplot)
```

El paquete **mlbench** se utiliza para cargar los datasets con los cuales trabajaremos en esta práctica. El paquete **caret** engloba más de 200 modelos de clasificación así como funciones útiles para el preprocesado de datos. A continuación cargamos el dataset BreastCancer de la librería **mlbench**. El objetivo de este dataset es describir si los pacientes tienen un cáncer benigno o maligno.

```{r  message=FALSE, warning=FALSE}
data(BreastCancer)
```

El dataset ya se encuentra disponible en el entorno, ahora solo podrás referenciarlo con la variable BreastCancer.

## Ejercicios

A continuación debe resolver los siguientes ejercicios, escribiendo el código en cada sección correspondiente después del comentario *#INSERTAR CÓDIGO AQUÍ*.

### Preprocesamiento de datos

Describa brevemente el dataset BreastCancer. Puede apoyarse en las funciones str(..) y summary (..). Como mínimo se espera que aporte la siguiente información:

* Número de ejemplos (observaciones).
* Número de variables.
* Tipo de variables.
* Distribuciones de datos por cada variable.
* Número de valores perdidos.

```{r}
#INSERTAR CÓDIGO AQUÍ
str(BreastCancer, width = 85, strict.width = "cut")
```

* Número de ejemplos (observaciones) = 699.
* Número de variables = 11.
* Tipo de variables. Consideramos el dataset BreastCancer definido sobre 10 variables descriptoras (9 discretas, 5 de ellas ordenadas; y una variable de tipo carácter) y una variable clase binaria \{benign, malignant\}.

* Distribuciones de datos por cada variable:

```{r}
#INSERTAR CÓDIGO AQUÍ
summary(BreastCancer)
```

* Número de valores perdidos.

```{r}
#INSERTAR CÓDIGO AQUÍ
sum(is.na(BreastCancer))
```

- En caso de ser necesario, elimine los valores perdidos (R los representa como NA).

```{r}
#INSERTAR CÓDIGO AQUÍ
BreastCancer <- na.omit(BreastCancer) 
any(is.na(BreastCancer))
```

Nota: en esta práctica, solo con la eliminación de registros es suficiente. Sin embargo, tenga en cuenta que la eliminación de registros que presentan valores perdidos no es una alternativa efectiva en la mayoría de los casos.

- Ahora, transforma todas las variables descriptoras a tipo numérico. Esto es necesario porque muchos métodos de clustering trabajan solamente con variables numéricas.

```{r}
#INSERTAR CÓDIGO AQUÍ
for (i in 1:(ncol(BreastCancer) - 1))
  BreastCancer[, i] <- as.numeric(as.character(BreastCancer[, i]))

str(BreastCancer, width = 85, strict.width = "cut")
```

- La variable "ID" representa un identificador que es único para cada paciente y no aporta ninguna información para el aprendizaje automático, por lo que podemos eliminarlo con seguridad.

```{r}
#INSERTAR CÓDIGO AQUÍ
BreastCancer$Id <- NULL
head(BreastCancer, 3)
```

- Debido a que los algoritmos de clustering se basan en el cómputo de valores de distancias entre los ejemplos, es ventajoso tener todas las variables en la misma escala para el cálculo de las distancias entre ejemplos. Convierta todos los atributos descriptores a una misma escala. 

Nota: se recomienda utilizar la función preProcess(.., method = "scale") de la librería caret.

```{r}
# INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("scale"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
str(BreastCancer.features, width = 85, strict.width = "cut")
```

Se ha eliminado la variable clase.

### Clustering con kMeans

Una vez que los datos han sido preprocesados, la librería ggplot2, la cual es automaticamente importada por la librería caret, nos permite visualizar los datos para comprobar cómo las diferentes variables afectan al tipo de cáncer.

```{r fig.align='center', out.width='70%'}
# En este caso solo estamos mostrando el gráfico para las variables Cl.thickness y Cell.size.
ggplot(BreastCancer, aes(Cl.thickness, Cell.size, color = Class)) + geom_point()
```

Se puede apreciar que considerando estos pares de variables se observan dos grupos claramente definidos.

- Pasemos ahora a ejecutar algoritmos de clustering sobre nuestros datos. Mediante un agrupamiento usando kmeans(..), encuentre el par de variables descriptoras a partir del cual se logra un mejor agrupamiento. Utilice 20 asignaciones aleatorias para inicializar los centroides y además fije el número de cluster igual al número de clases existentes en el dataset.

Nota: recuerde que la variable Class no se debe tener en cuenta a la hora de realizar el clustering, ya que estamos haciendo un aprendizaje no supervisado. El atributo Class es útil para usar una métrica de evaluación externa y así comprobar la calidad del agrupamiento.

```{r}
#INSERTAR CÓDIGO AQUÍ
set.seed(101)
km_clusters <- kmeans(BreastCancer.features[, c(2, 3)], centers = 2, nstart = 20)
km_clusters
```

- A partir de los resultados obtenidos, ¿Por qué se puede considerar un buen agrupamiento? Justifica de forma clara y concisa la elección del par de atributos. Puedes ayudarte con la funciones table(..) y ggplot(..) para justificar tu respuesta.

RESPUESTA:

El resultado obtenido del clustering representa el 78.2\% de la información del dataset.

```{r}
table(km_clusters$cluster, BreastCancer$Class)
```
Podemos apreciar que el primer cluster engloba a un mayor número de ejemplos benignos que malignos. También se puede apreciar que el segundo cluster engloba a más ejemplos malignos que benignos. Sin embargo, el primer cluster también alberga ejemplos del tipo maligno. Y el segundo cluster, también alberga ejemplos del tipo benigno. Por ello, podemos concluir que es un buen agrupamiento aunque los resultados son mejorables; el número de ejemplos benignos del segundo cluster es muy pequeño (4), casi se ha agrupado perfectamente.

Si representamos los resultados en una gráfica, se puede apreciar en ella que los clusters están bien definidos con una frontera clara entre ellos. Aunque, no hay mucha cohesión entre los valores de cada uno de los clusters y la separación entre ellos es mejorable.

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
fviz_cluster(km_clusters, data = BreastCancer.features[, c(2, 3)])
```
Para el clustering, se han elegido las variables *Cell.size* y *Cell.shape* porqué forman el par de variables más correlacionadas positivamente del dataset.

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
corrplot(cor(BreastCancer.features), method = "number", type = "lower")
```

- Mediante la función preprocess(..) de la librería caret, analice si se obtienen mejores resultados de agrupamiento con el par de atributos seleccionados anteriormente, pero en este caso haciendo inicialmente las siguientes combinaciones de transformaciones a los datos:

\newpage

  - c("center", "scale")
  - c("center", "scale", "YeoJohnson")
  - c("range")
  - c("range", "YeoJohnson")

Ayúdate de los resultados obtenidos con table y ggplot. 

- c("center", "scale")

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("center", "scale"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
set.seed(101)
km_clusters <- kmeans(BreastCancer.features[, c(2, 3)], centers = 2, nstart = 20)
table(km_clusters$cluster, BreastCancer$Class)
fviz_cluster(km_clusters, data = BreastCancer.features[, c(2, 3)])
```

El resultado no presenta mejoras.

- c("center", "scale", "YeoJohnson")

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], 
                                 method=c("center", "scale", "YeoJohnson"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
set.seed(101)
km_clusters <- kmeans(BreastCancer.features[, c(2, 3)], centers = 2, nstart = 20)
table(km_clusters$cluster, BreastCancer$Class)
fviz_cluster(km_clusters, data = BreastCancer.features[, c(2, 3)])
```

El resultado de agrupamiento presenta mejoras. El primer cluster ha reducido el número de ejemplos malignos a 5, y casi se ha agrupado perfectamente. Y aunque han aumentado el número de ejemplos benignos en el segundo cluster, este presenta mayor número de ejemplos malignos. Visualmente vemos que el segundo cluster se ha cohesionado mucho más, y la separación entre los clusters ha aumentado.

- c("range")

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("range"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
set.seed(101)
km_clusters <- kmeans(BreastCancer.features[, c(2, 3)], centers = 2, nstart = 20)
table(km_clusters$cluster, BreastCancer$Class)
fviz_cluster(km_clusters, data = BreastCancer.features[, c(2, 3)])
```

El resultado no presenta mejoras.

- c("range", "YeoJohnson")

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("range", "YeoJohnson"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
set.seed(101)
km_clusters <- kmeans(BreastCancer.features[, c(2, 3)], centers = 2, nstart = 20)
table(km_clusters$cluster, BreastCancer$Class)
fviz_cluster(km_clusters, data = BreastCancer.features[, c(2, 3)])
```

El resultado de agrupamiento presenta mejoras. Justamente, presenta las mismas mejoras que si escalamos con el método c("center", "scale", "YeoJohnson").

- A partir del par de variables seleccionadas, cree un agrupamiento conformado por tres variables; las dos variables previamente seleccionadas y otra variable seleccionada aleatoriamente entre las variables descriptoras restantes. Este proceso debes repetirlo cinco veces de tal manera que en cada ejecución la variable añadida no necesariamente sea la misma.

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("scale"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])

func <- function (var) { 
  new_data <- c("Cell.size", "Cell.shape", var)
  set.seed(101)
  km_clusters <- kmeans(BreastCancer.features[new_data], centers = 3, nstart = 20)
  fviz_cluster(km_clusters, data = BreastCancer.features[new_data])
}

vars <- c("Cl.thickness", "Marg.adhesion", "Epith.c.size", "Bare.nuclei", "Bl.cromatin")
```

\newpage

```{r fig.align='center', out.width='70%'}
func(vars[1])
func(vars[2])
```

\newpage

```{r fig.align='center', out.width='70%'}
func(vars[3])
func(vars[4])
```

\newpage

```{r fig.align='center', out.width='70%'}
func(vars[5])
```

- A partir de los resultados obtenidos responda lo siguiente, ¿Tiene alguna ventaja o desventaja incluir más variables? ¿Qué posible explicación puedes darle a los resultados?

En las gráficas anteriores, podemos visualizar que incluir más variables en un problema de alta dimensionalidad es una ventaja, aumenta la cohesión. Los algoritmos que se basan en distancias, como en este caso un algoritmo de clustering, tienen una maldición con los problemas de alta dimensionalidad, llamada la maldición de la dimensión. Esta maldición nos enseña que la variabilidad de las distancias entre nuestros datos disminuye exponencialmente con el número de dimensiones. 

Para mitigar el efecto de la maldición de la dimensión existen dos opciones:

- reducir el número de dimensiones.
- augmentar la cantidad de datos.

En este caso se ha elegido augmentar la cantidad de datos, y hemos visto visualmente es una buena medida. Ya que anteriormente hemos podido comprobar que cuando el número de dimensiones es muy alto, el espacio está casi vacío, hay poca cohesión. Por eso es tan difícil encontrar puntos que estén cerca.

### Clustering jerárquico

Como hemos visto, con kmeans debemos de especificar a priori el número de clusters que queremos obtener. Dado que el clustering es una tarea de aprendizaje no supervisado, en algunos casos puede que no tengamos
esa información a priori, por lo que necesitaremos de otro tipo de técnicas, como por ejemplo el agrupamiento jerárquico. Un agrupamiento jerárquico en R se puede realizar mediante la función hclust(..), para ello debemos especificar el tipo de método de aglomeración a usar.

-	Crea un agrupamiento jerárquico usando hclust(..), para ello debe encontrar el método de aglomeración que mejor agrupa los tipos de cáncer. Una vez encontrado el mejor método, gráfica un dendograma que muestre información sobre el agrupamiento.

Nota: Consulta la ayuda de la función hclust(..) para ver los tipos de métodos de aglomeración que soporta. También puedes ayudarte de las funciones table(..) y plot(..) para realizar tu elección final. Además, recuerda que no debes tener en cuenta la variable Class cuando ejecutas la función hclust(..).

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("scale"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
hc <- hclust(dist(BreastCancer.features, method = 'euclidean'), method = 'complete')
ggdendrogram(hc, rotate = FALSE, labels = FALSE, theme_dendro = TRUE) + 
  labs(title = "Cluster Dendrogram")
```

- ¿Por qué el dendograma tendrá tantos grupos en el nivel más bajo? ¿Coincide este número con algún otro? Justifica.

RESPUESTA: 

En el eje horizontal del dendograma tenemos cada una de las observaciones del dataset (699). Cada observación está asignada a un grupo, de modo que si tienen 699 observaciones, se tienen 699 clusters, cada uno con una sola observación.

En el eje vertical se representa la distancia euclídea que existe entre cada grupo a medida que éstos se van jerarquizando. Cada línea vertical del diagrama representa un agrupamiento que coincide con los puntos arropados por ésta, y como se ve en el dendrograma, estos van formándose progresivamente hasta tener un solo gran grupo determinado por la línea horizontal superior.

- ¿Por qué es necesario usar la función dist(..) al llamar a hclust(..) ?

RESPUESTA: 

A diferencia del método de kMeans, cuando efectuamos un clustering jerárquico es necesario determinar previamente los valores de distancia entre clusters (matriz de similaridad o distancia). Para ello, se utiliza la función *dist* de la función *hclust*. La medida predeterminada para la función dist es ‘Euclidiana’, sin embargo, se puede cambiar. También necesitamos especificar el método de vinculación que queremos usar (es decir, “completo”, “promedio”, etc.). En este caso se ha elegido el método completo.

La elección de la medida de distancia es muy importante para que el clustering tenga sentido. Con distintas medidas obtenemos distintos agrupamientos.

- ¿Por qué no es necesario usar set.seed(..) antes de llamar a hclust(..) ?

RESPUESTA:

No es necesaria la asignación aleatoria inicial de los centroides en un agrupamiento jerárquico, por el uso de la matriz de similaridad o distancia.

- Debido a que hclust puede crear un número elevado de clústers, utilice la función cutree(..) para fijar el número de clusters igual al número de clases existentes en el dataset.

```{r}
#INSERTAR CÓDIGO AQUÍ
hc <- hclust(dist(BreastCancer.features[, c(2, 3)], method = 'euclidean'), 
             method = 'complete')
clusters <- cutree(hc, k = 2)
BreastCancer.features$cluster <- clusters
```

Se ha escogido el método *euclidean* porqué se tendrá que comparar con los resultados de la ejecución de kMeans, y se ha utilizado la distancia Euclídea. Y también se ha elegido el método completo porqué es la medida más conservadora.

-¿Cuando hacemos esta última operación con la función cutree(..), el agrupamiento que obtenemos por hclust(..) es mejor al obtenido con kMeans en el ejercicio anterior?

RESPUESTA:

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
table(BreastCancer.features$cluster, BreastCancer$Class)
fviz_cluster(list(data = BreastCancer.features[, c(2, 3)], cluster = clusters))
```

Visualmente el agrupamiento es muy parecido al primer agrupamiento de kMeans. Pero al igual que hacíamos en el caso anterior, podemos mostrar el número de ejemplos de cada tipo de cáncer que cada grupo contiene con la ayuda de la función *table*. Podemos apreciar que el primer cluster engloba a un mayor número de ejemplos malignos, está peor agrupado que el primer cluster en kMeans. Y aunque el número de ejemplos benignos del segundo cluster es el mismo (4), existen menos ejemplos malignos, es menos representativo en el total de ejemplos del dataset. Así que, con estas observaciones podemos concluir que el agrupamiento es peor que el obtenido con kMeans.

Parece menos apropiado en datasets muy grandes, donde hay mucho ruido y datos atípicos. Podría ser porqué este algoritmo es más sensible al ruido y a los datos atípicos.