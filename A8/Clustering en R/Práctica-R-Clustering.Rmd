---
title: "Práctica en R del tema Clustering"
author: "Laura Rodríguez Navas"
date: "10/07/2020"
output:
  pdf_document: 
    fig_caption: yes
    keep_tex: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Parte introductoria de la práctica

Para la correcta ejecución de esta práctica, el estudiante debe verificar que los paquetes **mlbench** y **caret** están correctamente instalados en la plataforma R. Para instalar estos paquetes en R, solo debe ejecutar:

- **install.packages("mlbench")**
- **install.packages("caret")**

Además, para esta práctica se ha decidido que se necesitan los siguientes paquetes de R:

- **install.packages("lattice")**
- **install.packages("ggplot2")**
- **install.packages("factoextra")**
- **install.packages("ggdendro")**
- **install.packages("cluster")**

Luego de haber instalado los paquetes en R, pasamos a cargar en memoria las librerías que usaremos.

```{r message=FALSE, warning=FALSE}
library(mlbench)
library(lattice)
library(ggplot2)
library(caret)
library(factoextra)
library(ggdendro)
library(cluster)
```

El paquete **mlbench** se utiliza para cargar los datasets con los cuales trabajaremos en esta práctica. El paquete **caret** engloba más de 200 modelos de clasificación así como funciones útiles para el preprocesado de datos. A continuación cargamos el dataset BreastCancer de la librería **mlbench**. El objetivo de este dataset es describir si los pacientes tienen un cáncer benigno o maligno.

```{r  message=FALSE, warning=FALSE}
data(BreastCancer)
```

El dataset ya se encuentra disponible en el entorno, ahora solo podrás referenciarlo con la variable BreastCancer.

## Ejercicios

A continuación debe resolver los siguientes ejercicios, escribiendo el código en cada sección correspondiente después del comentario *#INSERTAR CÓDIGO AQUÍ*.

### Preprocesamiento de datos

Describa brevemente el dataset BreastCancer. Puede apoyarse en las funciones str(..) y summary (..). Como mínimo se espera que aporte la siguiente información:

* Número de ejemplos (observaciones).
* Número de variables.
* Tipo de variables.
* Distribuciones de datos por cada variable.
* Número de valores perdidos.

```{r}
#INSERTAR CÓDIGO AQUÍ
str(BreastCancer, width = 85, strict.width = "cut")
```

* Número de ejemplos (observaciones) = 699.
* Número de variables = 11.
* Tipo de variables. Consideramos el dataset BreastCancer definido sobre 10 variables descriptoras (9 discretas, 5 de ellas ordenadas; y una variable de tipo carácter) y una variable clase binaria \{benign, malignant\}.

* Distribuciones de datos por cada variable:

```{r}
#INSERTAR CÓDIGO AQUÍ
summary(BreastCancer)
```

\newpage
* Número de valores perdidos.

```{r}
#INSERTAR CÓDIGO AQUÍ
sum(is.na(BreastCancer))
```

- En caso de ser necesario, elimine los valores perdidos (R los representa como NA).

```{r}
#INSERTAR CÓDIGO AQUÍ
BreastCancer <- na.omit(BreastCancer) 
any(is.na(BreastCancer))
```

Nota: en esta práctica, solo con la eliminación de registros es suficiente. Sin embargo, tenga en cuenta que la eliminación de registros que presentan valores perdidos no es una alternativa efectiva en la mayoría de los casos.

- Ahora, transforma todas las variables descriptoras a tipo numérico. Esto es necesario porque muchos métodos de clustering trabajan solamente con variables numéricas.

```{r}
#INSERTAR CÓDIGO AQUÍ
for (i in 1:(ncol(BreastCancer) - 1))
  BreastCancer[, i] <- as.numeric(as.character(BreastCancer[, i]))

str(BreastCancer, width = 85, strict.width = "cut")
```

- La variable "ID" representa un identificador que es único para cada paciente y no aporta ninguna información para el aprendizaje automático, por lo que podemos eliminarlo con seguridad.

```{r}
#INSERTAR CÓDIGO AQUÍ
BreastCancer$Id <- NULL
head(BreastCancer, 3)
```

- Debido a que los algoritmos de clustering se basan en el cómputo de valores de distancias entre los ejemplos, es ventajoso tener todas las variables en la misma escala para el cálculo de las distancias entre ejemplos. Convierta todos los atributos descriptores a una misma escala. 

Nota: se recomienda utilizar la función preProcess(.., method = "scale") de la librería caret.

```{r}
# INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("scale"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
str(BreastCancer.features, width = 85, strict.width = "cut")
```

### Clustering con kMeans

Una vez que los datos han sido preprocesados, la librería ggplot2, la cual es automaticamente importada por la librería caret, nos permite visualizar los datos para comprobar cómo las diferentes variables afectan al tipo de cáncer.

```{r fig.align='center', out.width='70%'}
# En este caso solo estamos mostrando el gráfico para las variables Cl.thickness y Cell.size.
ggplot(BreastCancer, aes(Cl.thickness, Cell.size, color = Class)) + geom_point()
```

Se puede apreciar que considerando estos pares de variables se observan dos grupos claramente definidos.

- Pasemos ahora a ejecutar algoritmos de clustering sobre nuestros datos. Mediante un agrupamiento usando kmeans(..), encuentre el par de variables descriptoras a partir del cual se logra un mejor agrupamiento. Utilice 20 asignaciones aleatorias para inicializar los centroides y además fije el número de cluster igual al número de clases existentes en el dataset.

Nota: recuerde que la variable Class no se debe tener en cuenta a la hora de realizar el clustering, ya que estamos haciendo un aprendizaje no supervisado. El atributo Class es útil para usar una métrica de evaluación externa y así comprobar la calidad del agrupamiento.

```{r}
#INSERTAR CÓDIGO AQUÍ
set.seed(101)
km_clusters <- kmeans(BreastCancer.features, centers = 2, nstart = 20)
km_clusters
```

- A partir de los resultados obtenidos, ¿Por qué se puede considerar un buen agrupamiento? Justifica de forma clara y concisa la elección del par de atributos. Puedes ayudarte con la funciones table(..) y ggplot(..) para justificar tu respuesta.

RESPUESTA:

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
fviz_cluster(km_clusters, data = BreastCancer.features,
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot")
```
La propia función (*fviz_cluster*) determina las dos dimensiones, es decir, el par de variables más representativo en el dataset.

Si observamos la anterior figura, el agrupamiento no ha sido del todo bueno. Los clusters no acaban de verse bien diferenciados, y se puede observar un poco de *overlapping* entre ellos.

Para la validación de los clusters se utiliza el coeficiente de Silhouette: un valor de Si cercano a 1 indica que el cluster estará bien agrupado. Un valor de Si cercano a -1 indicará que el cluster estará mal agrupado.

A continuación vemos la representación del coeficiente de Silhouette:

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
sil <- silhouette(km_clusters$cluster, dist(BreastCancer.features))
fviz_silhouette(sil)
```

- Mediante la función preprocess(..) de la librería caret, analice si se obtienen mejores resultados de agrupamiento con el par de atributos seleccionados anteriormente, pero en este caso haciendo inicialmente las siguientes combinaciones de transformaciones a los datos:

  - c("center", "scale")
  - c("center", "scale", "YeoJohnson")
  - c("range")
  - c("range", "YeoJohnson")

Ayúdate de los resultados obtenidos con table y ggplot. 

- c("center", "scale")

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("center", "scale"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
set.seed(101)
km_clusters <- kmeans(BreastCancer.features, centers = 2, nstart = 20)
fviz_cluster(km_clusters, data = BreastCancer.features,
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot")
sil <- silhouette(km_clusters$cluster, dist(BreastCancer.features))
fviz_silhouette(sil)
```

El resultado de agrupamiento no presenta mejoras.

- c("center", "scale", "YeoJohnson")

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("center", "scale", "YeoJohnson"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
set.seed(101)
km_clusters <- kmeans(BreastCancer.features, centers = 2, nstart = 20)
fviz_cluster(km_clusters, data = BreastCancer.features,
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot")
sil <- silhouette(km_clusters$cluster, dist(BreastCancer.features))
fviz_silhouette(sil)
```

El resultado de agrupamiento sí presenta mejoras. Los clusters están más definidos. Ya no existe *overlapping* entre ellos. 

- c("range")

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("range"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
set.seed(101)
km_clusters <- kmeans(BreastCancer.features, centers = 2, nstart = 20)
fviz_cluster(km_clusters, data = BreastCancer.features,
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot")
sil <- silhouette(km_clusters$cluster, dist(BreastCancer.features))
fviz_silhouette(sil)
```

El resultado de agrupamiento sí presenta mejoras.

- c("range", "YeoJohnson")

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("range", "YeoJohnson"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])
set.seed(101)
km_clusters <- kmeans(BreastCancer.features, centers = 2, nstart = 20)
fviz_cluster(km_clusters, data = BreastCancer.features,
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot")
sil <- silhouette(km_clusters$cluster, dist(BreastCancer.features))
fviz_silhouette(sil)
```

El resultado de agrupamiento no presenta mejoras.

- A partir del par de variables seleccionadas, cree un agrupamiento conformado por tres variables; las dos variables previamente seleccionadas y otra variable seleccionada aleatoriamente entre las variables descriptoras restantes. Este proceso debes repetirlo cinco veces de tal manera que en cada ejecución la variable añadida no necesariamente sea la misma.

```{r fig.align='center', out.width='70%'}
#INSERTAR CÓDIGO AQUÍ
BreastCancer.scale <- preProcess(BreastCancer[, 1:9], method=c("scale"))
BreastCancer.features <- predict(BreastCancer.scale, BreastCancer[, 1:9])

func <- function (var) { 
  new_data <- c("Cell.size", "Cell.shape", var)
  set.seed(101)
  km_clusters <- kmeans(BreastCancer.features[new_data], centers = 2, nstart = 20)
  fviz_cluster(km_clusters, data = BreastCancer.features[, c("Cell.size", "Cell.shape")],
             ggtheme = theme_minimal(),
             main = "Partitioning Clustering Plot")
}

vars <- c("Cl.thickness", "Marg.adhesion", "Epith.c.size", "Bare.nuclei", "Bl.cromatin")

func(vars[1])
func(vars[2])
func(vars[3])
func(vars[4])
func(vars[5])
```

- A partir de los resultados obtenidos responda lo siguiente, ¿Tiene alguna ventaja o desventaja incluir más variables? ¿Qué posible explicación puedes darle a los resultados?

En todos los casos es una desventaja. Al incluir más variables los dos clusters se agrupan incorrectamente. Vemos como existe *overlapping*.

### Clustering jerárquico

Como hemos visto, con kmeans debemos de especificar a priori el número de clusters que queremos obtener. Dado que el clustering es una tarea de aprendizaje no supervisado, en algunos casos puede que no tengamos
esa información a priori, por lo que necesitaremos de otro tipo de técnicas, como por ejemplo el agrupamiento jerárquico. Un agrupamiento jerárquico en R se puede realizar mediante la función hclust(..), para ello debemos especificar el tipo de método de aglomeración a usar.

-	Crea un agrupamiento jerárquico usando hclust(..), para ello debe encontrar el método de aglomeración que mejor agrupa los tipos de cáncer. Una vez encontrado el mejor método, gráfica un dendograma que muestre información sobre el agrupamiento.

Nota: Consulta la ayuda de la función hclust(..) para ver los tipos de métodos de aglomeración que soporta. También puedes ayudarte de las funciones table(..) y plot(..) para realizar tu elección final. Además, recuerda que no debes tener en cuenta la variable Class cuando ejecutas la función hclust(..).

```{r}
#INSERTAR CÓDIGO AQUÍ
dendrogram <- hclust(dist(BreastCancer.features, method = 'euclidean'), method = 'ward.D')
ggdendrogram(dendrogram, rotate = FALSE, labels = FALSE, theme_dendro = TRUE) + labs(title = "Dendrograma")
```

- ¿Por qué el dendograma tendrá tantos grupos en el nivel más bajo? ¿Coincide este número con algún otro? Justifica.

RESPUESTA: 


- ¿Por qué es necesario usar la función dist(..) al llamar a hclust(..) ?


RESPUESTA:


- ¿Por qué no es necesario usar set.seed(..) antes de llamar a hclust(..) ?

RESPUESTA:



- Debido a que hclust puede crear un número elevado de clústers, utilice la función cutree(..) para fijar el número de clúster igual al número de clases existentes en el dataset.

```{r}
#INSERTAR CÓDIGO AQUÍ
agrupamientoJ <- hclust(dist(BreastCancer.features, method = 'euclidean'), method = 'ward.D')
clases_aj <- cutree(agrupamientoJ, k = 2)
BreastCancer.features$cluster <- clases_aj
clusters <- BreastCancer.features$cluster

ggplot(BreastCancer, aes(Cell.size, Cell.shape, color = as.factor(clusters))) + geom_point()
table(BreastCancer$Class, clusters)
```

-¿Cuando hacemos esta última operación con la función cutree(..), el agrupamiento que obtenemos por hclust(..) es mejor al obtenido con kMeans en el ejercicio anterior?

RESPUESTA:

