---
title: "Flujo de análisis en clasificación supervisada"
author: "Laura Rodríguez Navas"
date: "Septiembre 2020"
output:
  html_document: default
  pdf_document: 
    toc: yes
    fig_caption: yes
    fig_crop: no
    keep_tex: yes
    number_sections: yes
subtitle: Métodos supervisados
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Comenzamos cargando los paquetes necesarios.

```{r message=FALSE}
library(caret)
library(dplyr)
library(doParallel)
library(syuzhet)
library(ggcorrplot)
```

## Análisis Exploratorio de los Datos

Para la realización del ejercicio propuesto se ha elegido la competición en Kaggle: **Real or Not? NLP with Disaster Tweets**. El dataset de la competición se puede encontrar en el siguiente enlace: https://www.kaggle.com/c/nlp-getting-started/data. Este dataset, con 10.876 instancias, contiene 4 variables explicativas: **id**, **keyword**, **location** y **text**, y dos valores en la variable clase **target** (0 y 1). La variable clase es binaria, así que, vamos a aprender un modelo de clasificación binaria. El objetivo de este modelo será predecir si dado un tweet, este tweet trata sobre un desastre real o no. Si un tweet trata sobre un desastre real, se predice un 1. Si no, se predice un 0.

La métrica de evaluación esperada por la competición es [F1](https://www.kaggle.com/c/nlp-getting-started/overview/evaluation). Y se calcula de la siguiente manera:

```{r echo=FALSE, fig.align='center', out.width='30%'}
knitr::include_graphics('F1_score.png')
```

donde:

```{r echo=FALSE, fig.align='center', out.width='25%'}
knitr::include_graphics('F1_score_2.png')
```

La partición inicial train-test, no se tiene que realizar, ya que las instancias de train y test ya vienen definidas en el dataset de la competición (ficheros **train.csv** y **test.csv**). 

A continuación, cargaremos el conjunto de datos de train y test, nombrando los valores perdidos como **NA** para que los podamos tratar más adelante, y mostraremos sus dimensiones. 

```{r}
train <- read.csv("train.csv", na.strings=c("", "NA"))
test <- read.csv("test.csv", na.strings=c("", "NA"))
dim(train)
dim(test)
```

El conjunto de datos de train contiene 7613 instancias y el conjunto de datos de test contiene 3263 instancias. Cada instancia de estos conjuntos contiene la siguiente información:

* **id**: un identificador único para cada tweet. 
* **keyword**: una palabra clave del tweet.
* **location**: la ubicación desde la que se envió el tweet.
* **text**: el texto del tweet. 
* **target**: solo en el conjunto de datos de train porqué es la variable clase a predecir. Indica si un tweet es sobre un desastre real (1) o no (0). 

```{r}
str(train, width = 85, strict.width = "cut")
str(test, width = 85, strict.width = "cut")
```

### Variable *target*

Categorizamos la variable a predecir, ya que inicialmente es de tipo entero, y observamos su distribución.

```{r fig.align='center', out.width='70%'}
train$target <- as.factor(ifelse(train$target == 0, "No", "Yes"))
ggplot(train, aes(x=target)) + geom_bar(aes(fill=target))
```

La distribución no está muy sesgada y vemos que hay menos tweets que se refieren a desastres reales. La distribución de la variable a predecir está relativamente equilibrada, donde el 43% de las observaciones son desastrosas y el 57% no.

```{r}
sum(train$target == "Yes") / dim(train)[1] * 100
sum(train$target == "No") / dim(train)[1] * 100
```

Tampoco presenta un problema notable de *desbalanceo de clase* porqué contamos con muchas muestras del caso minoritario.

### Variable *keyword*

La variable **keyword** representa una palabra representativa de cada tweet, se muestran las primeras 10.

```{r}
train %>% select(keyword) %>% unique() %>% head(10)
```

Ahora veremos si la asociación de cada **keyword** con un sentimiento indica una relación con la variable a predecir. Para ello realizaremos un análisis de sentimientos de cada palabra clave. 

*El análisis de sentimientos es una técnica de *Machine Learning*, basada en el [procesado del lenguaje natural](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html), que pretende obtener información subjetiva de una serie de textos. Su aplicación es este caso, consiste en resolver si un tweet es real o no real en relación a un desastre.*

Para ello usaremos los paquetes **syuzhet**, **ggcorrplot** y **doParallel**.

- **syuzhet** cuenta con la función **get_nrc_sentiment** que calcula la presencia de los diferentes sentimientos dado un conjunto de textos. Los parámetros de esta función son:
  - **char_v**. Un vector de caracteres que en este caso contiene todas las palabras clave.
  - **language**. Define el lenguaje.
  - **cl**. Para análisis paralelo. Es opcional, pero en este caso lo usaremos porqué hay muchas palabras clave.
- **ggcorrplot** muestra una visualización gráfica de una matriz de correlación usando *ggplot2*.
- **doParallel** proporciona una computación paralela. Los parámetros de esta función son:
  - **makePSOCKcluster**. Crea un clúster de sockets paralelos.
  - **registerDoParallel**. Registra el número de *cores* que usará el clúster creado. 
  - **stopCluster**. Detiene la computación paralela.

Análisis de correlaciones entre **keyword** y **target**:

```{r fig.align='center', warning=FALSE, out.width='70%'}
#cl <- makePSOCKcluster(4, setup_strategy="sequential")
#registerDoParallel(cl)

#emocion.df <- get_nrc_sentiment(char_v = gsub("_", " ", train$keyword), language = "english", cl=cl)
#emocion.df <- emocion.df %>% data.frame(target = train$target)
#emocion.df$target <- as.numeric(emocion.df$target)

#cor(emocion.df) %>% 
  #ggcorrplot(lab = TRUE, 
             #title = "Matriz de correlación entre los sentimientos de keyword y target",
             #legend.title = "correlation")

#stopCluster(cl)
```

Al observar la matriz de correlaciones, se observa una correlación nula con cada uno de los sentimientos. Esto hace que esta variable explicativa no sea buena para hacer una predicción. 

### Variable *location*

La variable **location** representa la ubicación desde donde se generaron los tweets, se muestran las primeras 10.

```{r}
train %>% select(location) %>% unique() %>% head(10)
count(train %>% select(location) %>% unique())
```

En total hay 3342 ubicaciones. A continuación, mostramos las ubicaciones más frecuentes:

```{r fig.align='center', out.width='70%'}
location.freq <- table(unlist(train %>% select(location)))
location.freq[which(location.freq > 10)]
barplot(location.freq[which(location.freq>10)], las = 2,  
        ylab = "Frequency")
```

Del total de ubicaciones (3342), la mayoría de ellas cuenta con menos de 10 observaciones. Esto hace que esta variable explicativa tampoco sea buena para hacer una predicción. 

### Variable *text*

Unimos los conjuntos de train y test (*7613 + 3263 observaciones*) para poder analizar y extraer los sentimientos más adelante.

```{r}
complete_df <- bind_rows(train, test)
dim(complete_df)
```

Echaremos un vistazo más de cerca a las variables del nuevo conjunto de datos **complete_df**.

```{r}
summary(complete_df)
```

La variable **id** es solo un identificador único y la eliminaremos.

```{r}
complete_df$id <- NULL
```

Vemos los valores perdidos:

```{r}
colSums(sapply(complete_df, is.na))
```
Las variables explicativas **keyword** y **location** contienen valores perdidos. Sobretodo hay una gran cantidad de tweets, para los cuales falta su **location**. No existen valores perdidos para la variable explicativa **text**, tampoco para la variable a predecir **target**. Los 3263 valores perdidos de la variable a predecir provienen del conjunto de datos de test.

Nos ocuparemos de los valores perdidos más adelante.


