corpus_location <- tm_map(corpus_location, stemDocument)
corpus_location[[50]]$content
dtm_location <- DocumentTermMatrix(corpus_location)
dtm_location
dtm_location <- removeSparseTerms(dtm_location, 0.9975)
dtm_location
bag_of_words_location <- as.data.frame(as.matrix(dtm_location))
colnames(bag_of_words_location) <- paste0(colnames(bag_of_words_location), "_location")
bag_of_words_location <- bag_of_words_location[, !duplicated(colnames(bag_of_words_location))]
str(bag_of_words_location, list.len=10)
length(unique(train$text))
length(unique(test$text))
corpus_text <- Corpus(VectorSource(train_and_test$text))
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, tolower)
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, removePunctuation)
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, removeWords, stopwords("english"))
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, stemDocument)
corpus_text[[50]]$content
dtm_text <- DocumentTermMatrix(corpus_text)
dtm_text
dtm_text <- removeSparseTerms(dtm_text, 0.9975)
dtm_text
bag_of_words_text <- as.data.frame(as.matrix(dtm_text))
colnames(bag_of_words_text) <- paste0(colnames(bag_of_words_text), "_test")
bag_of_words_text <- bag_of_words_text[, !duplicated(colnames(bag_of_words_text))]
str(bag_of_words_text, list.len=10)
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # filas obtenidas por train
test_processed <- bag_of_words[7614:10876, ] # filas obtenidas por test
train_processed$keyword <- train$keyword
train_processed$target <- train$target
test_processed$keyword <- test$keyword
test_processed$target <- test$target
incomplete.cases <- which(!complete.cases((train_processed))) # Eliminamos los casos incompletos
train_processed[incomplete.cases, ] <- rep(0.0, ncol(train_processed))
incomplete.cases <- which(!complete.cases((train_processed))) # Eliminamos los casos incompletos
train_processed[incomplete.cases, ] <- rep(0, ncol(train_processed))
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(tm)
library(SnowballC)
knitr::include_graphics('F1_score.png')
knitr::include_graphics('F1_score_2.png')
train <- read.csv("train.csv", na.strings=c("", "NA"))
test <- read.csv("test.csv", na.strings=c("", "NA"))
dim(train)
dim(test)
str(train, width = 85, strict.width = "cut")
str(test, width = 85, strict.width = "cut")
train$target <- as.factor(train$target)
str(train, width = 85, strict.width = "cut")
ggplot(train, aes(x=target)) + geom_bar(aes(fill=target))
sum(train$target == "0") / dim(train)[1] * 100
sum(train$target == "1") / dim(train)[1] * 100
train$id <- NULL
test$id <- NULL
colSums(sapply(train, is.na))
colSums(sapply(test, is.na))
length(unique(train$keyword))
length(unique(test$keyword))
train$keyword <- as.factor(train$keyword)
test$keyword <- as.factor(test$keyword)
length(unique(train$location))
length(unique(test$location))
train_and_test <- rbind(train[, 1:3], test)
str(train_and_test,  width = 85, strict.width = "cut")
corpus_location <- Corpus(VectorSource(train_and_test$location))
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, tolower)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removePunctuation)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removeWords, stopwords("english"))
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, stemDocument)
corpus_location[[50]]$content
dtm_location <- DocumentTermMatrix(corpus_location)
dtm_location
dtm_location <- removeSparseTerms(dtm_location, 0.9975)
dtm_location
bag_of_words_location <- as.data.frame(as.matrix(dtm_location))
colnames(bag_of_words_location) <- paste0(colnames(bag_of_words_location), "_location")
bag_of_words_location <- bag_of_words_location[, !duplicated(colnames(bag_of_words_location))]
str(bag_of_words_location, list.len=10)
length(unique(train$text))
length(unique(test$text))
corpus_text <- Corpus(VectorSource(train_and_test$text))
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, tolower)
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, removePunctuation)
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, removeWords, stopwords("english"))
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, stemDocument)
corpus_text[[50]]$content
dtm_text <- DocumentTermMatrix(corpus_text)
dtm_text
dtm_text <- removeSparseTerms(dtm_text, 0.9975)
dtm_text
bag_of_words_text <- as.data.frame(as.matrix(dtm_text))
colnames(bag_of_words_text) <- paste0(colnames(bag_of_words_text), "_test")
bag_of_words_text <- bag_of_words_text[, !duplicated(colnames(bag_of_words_text))]
str(bag_of_words_text, list.len=10)
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # filas obtenidas por train
test_processed <- bag_of_words[7614:10876, ] # filas obtenidas por test
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(tm)
library(SnowballC)
knitr::include_graphics('F1_score.png')
knitr::include_graphics('F1_score_2.png')
train <- read.csv("train.csv", na.strings=c("", "NA"))
test <- read.csv("test.csv", na.strings=c("", "NA"))
dim(train)
dim(test)
str(train, width = 85, strict.width = "cut")
str(test, width = 85, strict.width = "cut")
train$target <- as.factor(train$target)
str(train, width = 85, strict.width = "cut")
ggplot(train, aes(x=target)) + geom_bar(aes(fill=target))
sum(train$target == "0") / dim(train)[1] * 100
sum(train$target == "1") / dim(train)[1] * 100
train$id <- NULL
test$id <- NULL
colSums(sapply(train, is.na))
colSums(sapply(test, is.na))
length(unique(train$keyword))
length(unique(test$keyword))
train$keyword <- as.factor(train$keyword)
test$keyword <- as.factor(test$keyword)
length(unique(train$location))
length(unique(test$location))
train_and_test <- rbind(train[, 1:3], test)
str(train_and_test,  width = 85, strict.width = "cut")
corpus_location <- Corpus(VectorSource(train_and_test$location))
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, tolower)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removePunctuation)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removeWords, stopwords("english"))
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, stemDocument)
corpus_location[[50]]$content
dtm_location <- DocumentTermMatrix(corpus_location)
dtm_location
dtm_location <- removeSparseTerms(dtm_location, 0.9975)
dtm_location
bag_of_words_location <- as.data.frame(as.matrix(dtm_location))
colnames(bag_of_words_location) <- paste0(colnames(bag_of_words_location), "_location")
bag_of_words_location <- bag_of_words_location[, !duplicated(colnames(bag_of_words_location))]
str(bag_of_words_location, list.len=10)
length(unique(train$text))
length(unique(test$text))
corpus_text <- Corpus(VectorSource(train_and_test$text))
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, tolower)
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, removePunctuation)
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, removeWords, stopwords("english"))
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, stemDocument)
corpus_text[[50]]$content
dtm_text <- DocumentTermMatrix(corpus_text)
dtm_text
dtm_text <- removeSparseTerms(dtm_text, 0.9975)
dtm_text
bag_of_words_text <- as.data.frame(as.matrix(dtm_text))
colnames(bag_of_words_text) <- paste0(colnames(bag_of_words_text), "_test")
bag_of_words_text <- bag_of_words_text[, !duplicated(colnames(bag_of_words_text))]
str(bag_of_words_text, list.len=10)
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # filas obtenidas por train
test_processed <- bag_of_words[7614:10876, ] # filas obtenidas por test
library(caret)
## KNN
num_folds <- trainControl(method = "cv", number = 5)
parameter_grid <- expand.grid(k = 1:3) # Explore values of `k` between 1 and 3.
knn <- train(
target ~ .,  # Use all variables in `train_processed` except `id`.
data = train_processed,
method = "knn",
trControl = num_folds,
tuneGrid = parameter_grid
)
knn
train_processed$keyword <- train$keyword
train_processed$target <- train$target
test_processed$keyword <- test$keyword
test_processed$target <- test$target
library(caret)
## KNN
num_folds <- trainControl(method = "cv", number = 5)
parameter_grid <- expand.grid(k = 1:3) # Explore values of `k` between 1 and 3.
knn <- train(
target ~ .,  # Use all variables in `train_processed` except `id`.
data = train_processed,
method = "knn",
trControl = num_folds,
tuneGrid = parameter_grid
)
knn
install.packages("glmnet")
library(dplyr)
library(ggplot2)
library(tm)
library(caret)
library(rpart)
library(glmnet)
library(MASS)
train <- read.csv("train.csv", stringsAsFactors = FALSE)
str(train)
test <- read.csv("test.csv", stringsAsFactors = FALSE)
str(test)
ggplot(train) +
geom_bar(aes(x = target), fill = "darkblue") +
scale_x_continuous(breaks = c(0, 1))
options(max.print = 250)
unique(train$keyword)
unique(test$keyword)
train$keyword <- as.factor(train$keyword)
test$keyword <- as.factor(test$keyword)
all.equal(levels(train$keyword), levels(test$keyword))
options(max.print = 100)
unique(train$location)
length(unique(train$location))
unique(test$location)
length(unique(test$location))
train_and_test <- rbind(train[, 1:4], test)
str(train_and_test)
corpus_location <- Corpus(VectorSource(train_and_test$location))
corpus_location
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, tolower)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removePunctuation)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removeWords, stopwords("english"))
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, stemDocument)
corpus_location[[50]]$content
dtm_location <- DocumentTermMatrix(corpus_location)
dtm_location
dtm_location <- removeSparseTerms(dtm_location, 0.9975) # Retain terms that appear in at least 0.25% of the observations.
dtm_location
bag_of_words_location <- as.data.frame(as.matrix(dtm_location))
colnames(bag_of_words_location) <- make.names(colnames(bag_of_words_location))
colnames(bag_of_words_location) <- paste0(colnames(bag_of_words_location), "_location")
str(bag_of_words_location, list.len = 20) # Display first 20 columns.
corpus_text <- Corpus(VectorSource(train_and_test$text))
corpus_text
corpus_text[[1]]$content
corpus_text <- tm_map(corpus_text, tolower)
corpus_text[[1]]$content
corpus_text <- tm_map(corpus_text, removePunctuation)
corpus_text[[1]]$content
corpus_text <- tm_map(corpus_text, removeWords, stopwords("english"))
corpus_text[[1]]$content
corpus_text <- tm_map(corpus_text, stemDocument)
corpus_text[[1]]$content
dtm_text <- DocumentTermMatrix(corpus_text)
dtm_text
dtm_text <- removeSparseTerms(dtm_text, 0.9975) # Retain terms that appear in at least 0.25% of the observations.
dtm_text
bag_of_words_text <- as.data.frame(as.matrix(dtm_text))
colnames(bag_of_words_text) <- make.names(colnames(bag_of_words_text))
colnames(bag_of_words_text) <- paste0(colnames(bag_of_words_text), "_text")
bag_of_words_text <- bag_of_words_text[, !duplicated(colnames(bag_of_words_text))]
str(bag_of_words_text, list.len = 20) # Display first 20 columns.
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # Rows 1 to 7613 were obtained from `train`.
test_processed <- bag_of_words[7614:10876, ] # Rows 7614 to 10876 were obtained from `test`.
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # Rows 1 to 7613 were obtained from `train`.
test_processed <- bag_of_words[7614:10876, ] # Rows 7614 to 10876 were obtained from `test`.
train_processed$id <- train$id
train_processed$keyword <- train$keyword
train_processed$target <- train$target
test_processed$id <- test$id
test_processed$keyword <- test$keyword
train_processed$target <- as.factor(train_processed$target)
num_folds <- trainControl(method = "cv", number = 5)
parameter_grid <- expand.grid(k = 1:5) # Explore values of `k` between 1 and 5.
grid_search <- train(
target ~ . - id,  # Use all variables in `train_processed` except `id`.
data = train_processed,
method = "knn",
trControl = num_folds,
tuneGrid = parameter_grid
)
grid_search
install.packages('rsconnect')
rsconnect::setAccountInfo(name='laura-rodriguez-navas',
token='E2F5EDBACC1B581750EA4E517A99B6D8',
secret='Ir86ff6avZr7ARM6NarIVyLYBIRDC+qSrC/kmMdz')
library(rsconnect)
rsconnect::deployApp('/Users/laurarodrigueznavas/BSC/shiny_server/apps/dorothea')
install.packages("hexbin")
install.packages("plotly")
library(rsconnect)
rsconnect::deployApp('/Users/laurarodrigueznavas/BSC/shiny_server/apps/dorothea')
.libPaths()
load("~/masterAI/A7/Flujo de análisis en clasificación supervisada R-caret/workspace.RData")
renv:::renv_paths_cache()
renv::status()
.libPaths()
.libPaths()
shiny::runApp('BSC/ShinyFUNKI')
runApp('BSC/ShinyFUNKI')
BiocManager::install("progeny")
runApp('BSC/ShinyFUNKI')
setwd("~/masterAI/A7/Flujo de análisis en clasificación supervisada R-caret")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringi)
library(tm)
library(irlba)
library(RColorBrewer)
library(gridExtra)
library(caret)
library(doParallel)
library(syuzhet)
library(ggcorrplot)
library(gbm)
train <- read.csv("train.csv", na.strings=c("", "NA"))
test <- read.csv("test.csv", na.strings=c("", "NA"))
dim(train)
dim(test)
str(train, width = 85, strict.width = "cut")
str(test, width = 85, strict.width = "cut")
train$target <- as.factor(ifelse(train$target == 0, "No", "Yes"))
ggplot(train, aes(x=target)) + geom_bar(aes(fill=target))
sum(train$target == "Yes") / dim(train)[1] * 100
sum(train$target == "No") / dim(train)[1] * 100
train %>% select(keyword) %>% unique() %>% head(10)
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
emotion.df <- get_nrc_sentiment(char_v = gsub("_", " ", train$keyword),
language = "english", cl=cl)
emotion.df <- emotion.df %>% data.frame(target = train$target)
emotion.df$target <- as.numeric(emotion.df$target)
cor(emotion.df) %>%
ggcorrplot(lab = TRUE,
title = "Matriz de correlación entre \nkeyword y target",
legend.title = "correlation")
stopCluster(cl)
train %>% select(location) %>% unique() %>% head(10)
count(train %>% select(location) %>% unique())
location.freq <- table(unlist(train %>% select(location)))
location.freq[which(location.freq > 10)]
barplot(location.freq[which(location.freq>10)], las = 2,
ylab = "frequency")
train$id <- NULL
test$id <- NULL
complete_df <- bind_rows(train, test)
str(complete_df, width = 85, strict.width = "cut")
colSums(sapply(complete_df, is.na))
myCorpus <- Corpus(VectorSource(complete_df$text))
myCorpus
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeUsername <- function(x) gsub("@[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeUsername))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myStopWords <- c((stopwords('english')),
c("really", "tweets", "saw", "just", "feel", "may", "us", "rt", "every", "one",
"amp", "like", "will", "got", "new", "can", "still", "back", "top", "much",
"near", "im", "see", "via", "get", "now", "come", "oil", "let", "god", "want",
"pm", "last", "hope", "since", "everyone", "food", "content", "always", "th",
"full", "found", "dont", "look", "cant", "mh", "lol", "set", "old", "service",
"city", "home", "live", "night", "news", "say", "video", "people", "ill",
"way",  "please", "years", "take", "homes", "read", "man", "next", "cross",
"boy", "bad", "ass"))
head(myStopWords, 30)
myCorpus <- tm_map(myCorpus, removeWords, myStopWords)
removeSingle <- function(x) gsub(" . ", " ", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeSingle))
myCorpus <- tm_map(myCorpus, stripWhitespace)
complete.tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths= c(4, Inf)))
complete.tdm
complete.tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths= c(4, Inf)))
complete.tdm
complete.tdm <- removeSparseTerms(complete.tdm, sparse = .95)
complete.tdm
complete.tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths= c(4, Inf)))
complete.tdm
new.tdm <- removeSparseTerms(complete.tdm, sparse = .9975)
new.tdm
complete.tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths= c(4, Inf)))
complete.tdm
complete.tdm <- removeSparseTerms(complete.tdm, sparse = .9975)
complete.tdm
complete.term.matrix <- as.matrix(t(complete.tdm))
dim(complete.term.matrix)
complete.tdm <- removeSparseTerms(complete.tdm, sparse = .9975)
complete.tdm
complete.term.matrix <- as.matrix(t(complete.tdm))
dim(complete.term.matrix)
complete.term.matrix <- as.matrix(t(complete.tdm))
dim(complete.term.matrix)
incomplete.cases <- which(!complete.cases(complete.term.matrix))
which(!complete.cases(complete.term.matrix))
complete.term.matrix <- as.matrix(t(complete.tdm))
which(!complete.cases(complete.term.matrix))
complete.term.matrix <- as.matrix(t(complete.tdm))
which(!complete.cases(complete.term.matrix))
complete.term.matrix <- as.matrix(t(complete.tdm))
count(which(!complete.cases(complete.term.matrix)))
complete.term.matrix <- as.matrix(t(complete.tdm))
which(!complete.cases(complete.term.matrix))
complete.term.matrix <- as.matrix(t(complete.tdm))
which(!complete.cases(complete.term.matrix))
complete_irlba <- irlba(t(complete.term.matrix), nv = 150, maxit = 600)
complete_irlba$v[1:10, 1:5]
View(complete_irlba)
View(complete.term.matrix)
complete.term.matrix <- as.matrix(t(complete.tdm))
which(!complete.cases(complete.term.matrix))
complete.svd <- data.frame(target = complete_df$target, complete_irlba$v)
train.df <- complete.svd[1:7613, ]
test.df <- complete.svd[7614:10876, -1]
dim(train.df)
dim(test.df)
names(train.df) <- make.names(names(train.df))
names(test.df) <- make.names(names(test.df))
complete.term.matrix <- as.matrix(t(complete.tdm))
which(!complete.cases(complete.term.matrix))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringi)
library(tm)
library(irlba)
library(RColorBrewer)
library(gridExtra)
library(caret)
library(doParallel)
library(syuzhet)
library(ggcorrplot)
library(gbm)
train <- read.csv("train.csv", na.strings=c("", "NA"))
test <- read.csv("test.csv", na.strings=c("", "NA"))
dim(train)
dim(test)
str(train, width = 85, strict.width = "cut")
str(test, width = 85, strict.width = "cut")
train$target <- as.factor(ifelse(train$target == 0, "No", "Yes"))
ggplot(train, aes(x=target)) + geom_bar(aes(fill=target))
sum(train$target == "Yes") / dim(train)[1] * 100
sum(train$target == "No") / dim(train)[1] * 100
train %>% select(keyword) %>% unique() %>% head(10)
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
emotion.df <- get_nrc_sentiment(char_v = gsub("_", " ", train$keyword),
language = "english", cl=cl)
emotion.df <- emotion.df %>% data.frame(target = train$target)
emotion.df$target <- as.numeric(emotion.df$target)
cor(emotion.df) %>%
ggcorrplot(lab = TRUE,
title = "Matriz de correlación entre \nkeyword y target",
legend.title = "correlation")
stopCluster(cl)
train %>% select(location) %>% unique() %>% head(10)
count(train %>% select(location) %>% unique())
location.freq <- table(unlist(train %>% select(location)))
location.freq[which(location.freq > 10)]
barplot(location.freq[which(location.freq>10)], las = 2,
ylab = "frequency")
train$id <- NULL
test$id <- NULL
complete_df <- bind_rows(train, test)
str(complete_df, width = 85, strict.width = "cut")
colSums(sapply(complete_df, is.na))
myCorpus <- Corpus(VectorSource(complete_df$text))
myCorpus
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeUsername <- function(x) gsub("@[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeUsername))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myStopWords <- c((stopwords('english')),
c("really", "tweets", "saw", "just", "feel", "may", "us", "rt", "every", "one",
"amp", "like", "will", "got", "new", "can", "still", "back", "top", "much",
"near", "im", "see", "via", "get", "now", "come", "oil", "let", "god", "want",
"pm", "last", "hope", "since", "everyone", "food", "content", "always", "th",
"full", "found", "dont", "look", "cant", "mh", "lol", "set", "old", "service",
"city", "home", "live", "night", "news", "say", "video", "people", "ill",
"way",  "please", "years", "take", "homes", "read", "man", "next", "cross",
"boy", "bad", "ass"))
head(myStopWords, 30)
myCorpus <- tm_map(myCorpus, removeWords, myStopWords)
removeSingle <- function(x) gsub(" . ", " ", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeSingle))
myCorpus <- tm_map(myCorpus, stripWhitespace)
complete.tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths= c(4, Inf)))
complete.tdm
complete.tdm <- removeSparseTerms(complete.tdm, sparse = .9975)
complete.tdm
complete.term.matrix <- as.matrix(t(complete.tdm))
which(!complete.cases(complete.term.matrix))
