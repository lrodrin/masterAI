bag_of_words_text <- as.data.frame(as.matrix(dtm_text))
colnames(bag_of_words_text) <- paste0(colnames(bag_of_words_text), "_test")
bag_of_words_text <- bag_of_words_text[, !duplicated(colnames(bag_of_words_text))]
str(bag_of_words_text, list.len=10)
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # filas obtenidas por train
test_processed <- bag_of_words[7614:10876, ] # filas obtenidas por test
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(tm)
library(SnowballC)
knitr::include_graphics('F1_score.png')
knitr::include_graphics('F1_score_2.png')
train <- read.csv("train.csv", na.strings=c("", "NA"))
test <- read.csv("test.csv", na.strings=c("", "NA"))
dim(train)
dim(test)
str(train, width = 85, strict.width = "cut")
str(test, width = 85, strict.width = "cut")
train$target <- as.factor(train$target)
str(train, width = 85, strict.width = "cut")
ggplot(train, aes(x=target)) + geom_bar(aes(fill=target))
sum(train$target == "0") / dim(train)[1] * 100
sum(train$target == "1") / dim(train)[1] * 100
train$id <- NULL
test$id <- NULL
colSums(sapply(train, is.na))
colSums(sapply(test, is.na))
length(unique(train$keyword))
length(unique(test$keyword))
train$keyword <- as.factor(train$keyword)
test$keyword <- as.factor(test$keyword)
length(unique(train$location))
length(unique(test$location))
train_and_test <- rbind(train[, 1:3], test)
str(train_and_test,  width = 85, strict.width = "cut")
corpus_location <- Corpus(VectorSource(train_and_test$location))
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, tolower)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removePunctuation)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removeWords, stopwords("english"))
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, stemDocument)
corpus_location[[50]]$content
dtm_location <- DocumentTermMatrix(corpus_location)
dtm_location
dtm_location <- removeSparseTerms(dtm_location, 0.9975)
dtm_location
bag_of_words_location <- as.data.frame(as.matrix(dtm_location))
colnames(bag_of_words_location) <- paste0(colnames(bag_of_words_location), "_location")
bag_of_words_location <- bag_of_words_location[, !duplicated(colnames(bag_of_words_location))]
str(bag_of_words_location, list.len=10)
length(unique(train$text))
length(unique(test$text))
corpus_text <- Corpus(VectorSource(train_and_test$text))
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, tolower)
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, removePunctuation)
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, removeWords, stopwords("english"))
corpus_text[[50]]$content
corpus_text <- tm_map(corpus_text, stemDocument)
corpus_text[[50]]$content
dtm_text <- DocumentTermMatrix(corpus_text)
dtm_text
dtm_text <- removeSparseTerms(dtm_text, 0.9975)
dtm_text
bag_of_words_text <- as.data.frame(as.matrix(dtm_text))
colnames(bag_of_words_text) <- paste0(colnames(bag_of_words_text), "_test")
bag_of_words_text <- bag_of_words_text[, !duplicated(colnames(bag_of_words_text))]
str(bag_of_words_text, list.len=10)
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # filas obtenidas por train
test_processed <- bag_of_words[7614:10876, ] # filas obtenidas por test
library(caret)
## KNN
num_folds <- trainControl(method = "cv", number = 5)
parameter_grid <- expand.grid(k = 1:3) # Explore values of `k` between 1 and 3.
knn <- train(
target ~ .,  # Use all variables in `train_processed` except `id`.
data = train_processed,
method = "knn",
trControl = num_folds,
tuneGrid = parameter_grid
)
knn
train_processed$keyword <- train$keyword
train_processed$target <- train$target
test_processed$keyword <- test$keyword
test_processed$target <- test$target
library(caret)
## KNN
num_folds <- trainControl(method = "cv", number = 5)
parameter_grid <- expand.grid(k = 1:3) # Explore values of `k` between 1 and 3.
knn <- train(
target ~ .,  # Use all variables in `train_processed` except `id`.
data = train_processed,
method = "knn",
trControl = num_folds,
tuneGrid = parameter_grid
)
knn
install.packages("glmnet")
library(dplyr)
library(ggplot2)
library(tm)
library(caret)
library(rpart)
library(glmnet)
library(MASS)
train <- read.csv("train.csv", stringsAsFactors = FALSE)
str(train)
test <- read.csv("test.csv", stringsAsFactors = FALSE)
str(test)
ggplot(train) +
geom_bar(aes(x = target), fill = "darkblue") +
scale_x_continuous(breaks = c(0, 1))
options(max.print = 250)
unique(train$keyword)
unique(test$keyword)
train$keyword <- as.factor(train$keyword)
test$keyword <- as.factor(test$keyword)
all.equal(levels(train$keyword), levels(test$keyword))
options(max.print = 100)
unique(train$location)
length(unique(train$location))
unique(test$location)
length(unique(test$location))
train_and_test <- rbind(train[, 1:4], test)
str(train_and_test)
corpus_location <- Corpus(VectorSource(train_and_test$location))
corpus_location
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, tolower)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removePunctuation)
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, removeWords, stopwords("english"))
corpus_location[[50]]$content
corpus_location <- tm_map(corpus_location, stemDocument)
corpus_location[[50]]$content
dtm_location <- DocumentTermMatrix(corpus_location)
dtm_location
dtm_location <- removeSparseTerms(dtm_location, 0.9975) # Retain terms that appear in at least 0.25% of the observations.
dtm_location
bag_of_words_location <- as.data.frame(as.matrix(dtm_location))
colnames(bag_of_words_location) <- make.names(colnames(bag_of_words_location))
colnames(bag_of_words_location) <- paste0(colnames(bag_of_words_location), "_location")
str(bag_of_words_location, list.len = 20) # Display first 20 columns.
corpus_text <- Corpus(VectorSource(train_and_test$text))
corpus_text
corpus_text[[1]]$content
corpus_text <- tm_map(corpus_text, tolower)
corpus_text[[1]]$content
corpus_text <- tm_map(corpus_text, removePunctuation)
corpus_text[[1]]$content
corpus_text <- tm_map(corpus_text, removeWords, stopwords("english"))
corpus_text[[1]]$content
corpus_text <- tm_map(corpus_text, stemDocument)
corpus_text[[1]]$content
dtm_text <- DocumentTermMatrix(corpus_text)
dtm_text
dtm_text <- removeSparseTerms(dtm_text, 0.9975) # Retain terms that appear in at least 0.25% of the observations.
dtm_text
bag_of_words_text <- as.data.frame(as.matrix(dtm_text))
colnames(bag_of_words_text) <- make.names(colnames(bag_of_words_text))
colnames(bag_of_words_text) <- paste0(colnames(bag_of_words_text), "_text")
bag_of_words_text <- bag_of_words_text[, !duplicated(colnames(bag_of_words_text))]
str(bag_of_words_text, list.len = 20) # Display first 20 columns.
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # Rows 1 to 7613 were obtained from `train`.
test_processed <- bag_of_words[7614:10876, ] # Rows 7614 to 10876 were obtained from `test`.
bag_of_words <- cbind(bag_of_words_location, bag_of_words_text)
train_processed <- bag_of_words[1:7613, ] # Rows 1 to 7613 were obtained from `train`.
test_processed <- bag_of_words[7614:10876, ] # Rows 7614 to 10876 were obtained from `test`.
train_processed$id <- train$id
train_processed$keyword <- train$keyword
train_processed$target <- train$target
test_processed$id <- test$id
test_processed$keyword <- test$keyword
train_processed$target <- as.factor(train_processed$target)
num_folds <- trainControl(method = "cv", number = 5)
parameter_grid <- expand.grid(k = 1:5) # Explore values of `k` between 1 and 5.
grid_search <- train(
target ~ . - id,  # Use all variables in `train_processed` except `id`.
data = train_processed,
method = "knn",
trControl = num_folds,
tuneGrid = parameter_grid
)
grid_search
install.packages('rsconnect')
rsconnect::setAccountInfo(name='laura-rodriguez-navas',
token='E2F5EDBACC1B581750EA4E517A99B6D8',
secret='Ir86ff6avZr7ARM6NarIVyLYBIRDC+qSrC/kmMdz')
library(rsconnect)
rsconnect::deployApp('/Users/laurarodrigueznavas/BSC/shiny_server/apps/dorothea')
install.packages("hexbin")
install.packages("plotly")
library(rsconnect)
rsconnect::deployApp('/Users/laurarodrigueznavas/BSC/shiny_server/apps/dorothea')
.libPaths()
load("~/masterAI/A7/Flujo de an치lisis en clasificaci칩n supervisada R-caret/workspace.RData")
renv:::renv_paths_cache()
renv::status()
.libPaths()
.libPaths()
load("~/masterAI/A7/Flujo de an치lisis en clasificaci칩n supervisada R-caret/workspace.RData")
library(caret)
partitions <- 5
reteats <- 3
set.seed(123)
cv.folds <- createMultiFolds(train.df$target, k=5, times=3)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE).
gc()
cl <- makePSOCKcluster(4, setup_strategy="sequential")
library(caret)
library(doParallel)
partitions <- 5
reteats <- 3
set.seed(123)
cv.folds <- createMultiFolds(train.df$target, k=5, times=3)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE).
gc()
# enable parallel processing
cl <- makePSOCKcluster(4, setup_strategy="sequential")
library(caret)
library(doParallel)
partitions <- 5
reteats <- 3
set.seed(123)
cv.folds <- createMultiFolds(train.df$target, k=5, times=3)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE).
gc()
# enable parallel processing
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
library(caret)
library(doParallel)
partitions <- 5
reteats <- 3
set.seed(123)
cv.folds <- createMultiFolds(train.df$target, k=5, times=3)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE).
gc()
# enable parallel processing
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_glmnet <- train(target ~ ., data=train.df,
method="glmnet",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_glmnet
library(caret)
library(doParallel)
partitions <- 5
reteats <- 3
set.seed(123)
cv.folds <- createMultiFolds(train.df$target, k=5, times=3)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE).
gc()
# enable parallel processing
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_glmnet <- train(target ~ ., data=train.df,
method="glmnet",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_glmnet
library(caret)
library(doParallel)
partitions <- 5
reteats <- 3
set.seed(123)
cv.folds <- createMultiFolds(train.df$target, k=5, times=3)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE).
gc()
# enable parallel processing
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_glmnet <- train(target ~ ., data=train.df,
method="glmnet",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_glmnet
model_glmnet <- train(target ~ ., data=train.df,
method="glmnet",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE).
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE)
library(caret)
library(doParallel)
partitions <- 5
reteats <- 3
set.seed(123)
cv.folds <- createMultiFolds(train.df$target, k=5, times=3)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, summaryFunction = twoClassSummary,
classProbs = TRUE, allowParallel = TRUE,
savePredictions = TRUE)
gc()
# enable parallel processing
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_glmnet <- train(target ~ ., data=train.df,
method="glmnet",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_glmnet
library(caret)
library(doParallel)
partitions <- 5
reteats <- 3
set.seed(123)
cv.folds <- createMultiFolds(train.df$target, k=5, times=3)
cv.cntrl <- trainControl(method = "repeatecv", number = 5, repeats = 3,
index = cv.folds, allowParallel = TRUE, returnResamp="final",
verboseIter=FALSE)
gc()
# enable parallel processing
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_glmnet <- train(target ~ ., data=train.df,
method="glmnet",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_glmnet
stopCluster(cl)
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_rf <- train(target ~ ., data=train.df,
method="rf",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_rf
stopCluster(cl)
# RF
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_rf <- train(target ~ ., data=train.df,
method="rf",
metric="Accuracy",
trControl=cv.cntrl,
tuneGrid = expand.grid(mtry=c(3, 4, 5, 7)))
model_rf
stopCluster(cl)
# GBM
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
cv.grid <- expand.grid(interaction.depth=c(1, 2),
n.trees=100,
shrinkage=c(0.001, 0.01, 0.1),
n.minobsinnode=c(2, 5, 25))
model_gbm <- train(target ~ ., data=train.df,
method="gbm",
metric="Accuracy",
trControl=cv.cntrl,
tuneGrid=cv.grid,
distribution="adaboost",
verbose=FALSE)
library(gbm)
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
cv.grid <- expand.grid(interaction.depth=c(1, 2),
n.trees=100,
shrinkage=c(0.001, 0.01, 0.1),
n.minobsinnode=c(2, 5, 25))
model_gbm <- train(target ~ ., data=train.df,
method="gbm",
metric="Accuracy",
trControl=cv.cntrl,
tuneGrid=cv.grid,
distribution="adaboost",
verbose=FALSE)
model_gbm
stopCluster(cl)
# KNN
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
cv.grid <- expand.grid(k=1:3)
model_gbm <- train(target ~ ., data=train.df,
method="knn",
metric="Accuracy",
trControl=cv.cntrl,
tuneGrid=cv.grid)
model_gbm
stopCluster(cl)
# NB
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_glmnet <- train(target ~ ., data=train.df,
method="nb",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_glmnet
stopCluster(cl)
# NB
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_nb <- train(target ~ ., data=train.df,
method="nb",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_nb
stopCluster(cl)
# SVM
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
cv.grid <- expand.grid(sigma=c(0.001, 0.01, 0.1, 0.5, 1),
C=c(1, 20, 50, 100))
model_svm <- train(target ~ ., data=train.df,
method="svmRadial",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_svm
stopCluster(cl)
save.image("~/masterAI/A7/Flujo de ana패lisis en clasificacio패n supervisada R-caret/workspace_3.RData")
# KNN
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
cv.grid <- expand.grid(k=1:3)
model_knn <- train(target ~ ., data=train.df,
method="knn",
metric="Accuracy",
trControl=cv.cntrl,
tuneGrid=cv.grid)
model_knn
stopCluster(cl)
# GLMNET
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
model_glmnet <- train(target ~ ., data=train.df,
method="glmnet",
metric="Accuracy",
trControl=cv.cntrl,
tuneLenght=15)
model_glmnet
stopCluster(cl)
# GBM
library(gbm)
cl <- makePSOCKcluster(4, setup_strategy="sequential")
registerDoParallel(cl)
cv.grid <- expand.grid(interaction.depth=c(1, 2),
n.trees=100,
shrinkage=c(0.001, 0.01, 0.1),
n.minobsinnode=c(2, 5, 25))
model_gbm <- train(target ~ ., data=train.df,
method="gbm",
metric="Accuracy",
trControl=cv.cntrl,
tuneGrid=cv.grid,
distribution="adaboost",
verbose=FALSE)
model_gbm
stopCluster(cl)
models <- list(KNN=model_knn, NB=model_nb, logistic=model_glmnet,
RF=model_rf, boosting=model_gbm, SVMRadial=model_svm)
resamps <- resamples(models)
summary(resamps)
submission <- read.csv("sample_submission.csv")
setwd("~/masterAI/A7/Flujo de an치lisis en clasificaci칩n supervisada R-caret")
submission <- read.csv("sample_submission.csv")
pred_svm <- predict(data=model_svm, test.df, type="raw")
pred_svm <- predict(model_svm, test.df, type="raw")
submission$target <- ifelse(pred_svm=="No", 0, 1)
head(submission)
# predict on test with SVMRadial
pred_svm <- predict(model_svm, test.df, type="raw")
# submission
submission <- read.csv("sample_submission.csv")
submission$target <- ifelse(pred_svm=="No", 0, 1)
head(submission)
write.csv(submission, "submission.csv", row.names=FALSE)
