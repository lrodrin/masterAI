summary(resamps)
View(nb)
nb[["results"]][["Accuracy"]]
nb[["results"]][["Kappa"]]
View(resamps)
resamps[["values"]][["NB~Accuracy"]]
resamps[["values"]][["NB~Kappa"]]
resamps <- resamples(list(NB = nb,
NN = nn,
SVM = svm), resamples = "final")
resamps
summary(resamps)
View(resamps)
itControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1,
returnResamp = "final")
# Naive Bayes
library(naivebayes)
set.seed(825)
nb <- train(Class ~ ., data = data_training,
method = "naive_bayes",
trControl = fitControl)
nb[["resample"]][["Accuracy"]]
nb[["resample"]][["Kappa"]]
# Decision Tree
# Neural Network
library(nnet)
set.seed(825)
nn <- train(Class ~ ., data = data_training,
method = "nnet",
trControl = fitControl)
nn[["resample"]][["Accuracy"]]
nn[["resample"]][["Kappa"]]
# Nearest Neighbour
# SVM (linear kernel)
library(kernlab)
set.seed(825)
svm <- train(Class ~ ., data = data_training,
method = "svmLinear",
trControl = fitControl)
svm[["resample"]][["Accuracy"]]
svm[["resample"]][["Kappa"]]
resamps <- resamples(list(NB = nb,
NN = nn,
SVM = svm), resamples = "final")
resamps
summary(resamps)
View(resamps)
View(nb)
# SVM (linear kernel)
library(kernlab)
library(alpha)
set.seed(825)
svm <- train(Class ~ ., data = data_training,
method = "svmLinear",
trControl = fitControl)
resamps <- resamples(list(NB = nb,
NN = nn,
SVM = svm), resamples = "final")
resamps
summary(resamps)
#1. Carga los datos en R. Nombra las columnas para identificar major el tablero,
#como si se visitarán de izquierda a derecha y de arriba a abajo.
data <- read.table("tic-tac-toe.data.txt", header=FALSE, sep=",")
names(data) <- c("top-left-square", "top-middle-square", "top-right-square",
"middle-left-square", "middle-middle-square", "middle-right-square", "bottom-left-square",
"bottom-middle-square", "bottom-right-square", "Class")
#Comprueba si hay valores faltantes.
any(is.na(data))
#2. Lee la sección "data splitting" de la web de “caret”. A continuación parte los
#datos en 70% para entrenamiento y 30% de test manteniendo la proporción original de
#clases.
library(caret)
set.seed(998)
inTraining <- createDataPartition(data$Class, p=.7, list=FALSE)
data_training <- data[ inTraining,]
data_testing  <- data[-inTraining,]
#3.
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1)
# Naive Bayes
library(naivebayes)
set.seed(825)
nb <- train(Class ~ ., data = data_training,
method = "naive_bayes",
trControl = fitControl)
# Decision Tree
# Neural Network
library(nnet)
set.seed(825)
nn <- train(Class ~ ., data = data_training,
method = "nnet",
trControl = fitControl)
# Nearest Neighbour
# SVM (linear kernel)
library(kernlab)
set.seed(825)
svm <- train(Class ~ ., data = data_training,
method = "svmLinear",
trControl = fitControl)
resamps <- resamples(list(NB = nb,
NN = nn,
SVM = svm), resamples = "final")
resamps
summary(resamps)
?rpart
#1. Carga los datos en R. Nombra las columnas para identificar major el tablero,
#como si se visitarán de izquierda a derecha y de arriba a abajo.
data <- read.table("tic-tac-toe.data.txt", header=FALSE, sep=",")
names(data) <- c("top-left-square", "top-middle-square", "top-right-square",
"middle-left-square", "middle-middle-square", "middle-right-square", "bottom-left-square",
"bottom-middle-square", "bottom-right-square", "Class")
#Comprueba si hay valores faltantes.
any(is.na(data))
#2. Lee la sección "data splitting" de la web de “caret”. A continuación parte los
#datos en 70% para entrenamiento y 30% de test manteniendo la proporción original de
#clases.
library(caret)
set.seed(998)
inTraining <- createDataPartition(data$Class, p=.7, list=FALSE)
data_training <- data[ inTraining,]
data_testing  <- data[-inTraining,]
#3.
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 1)
# Naive Bayes
library(naivebayes)
set.seed(825)
nb <- train(Class ~ ., data = data_training,
method = "naive_bayes",
trControl = fitControl)
# Decision Tree
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl)
# Neural Network
library(nnet)
set.seed(825)
nn <- train(Class ~ ., data = data_training,
method = "nnet",
trControl = fitControl)
# Nearest Neighbour
# SVM (linear kernel)
library(kernlab)
set.seed(825)
svm <- train(Class ~ ., data = data_training,
method = "svmLinear",
trControl = fitControl)
resamps <- resamples(list(NB = nb,
DT = dt,
NN = nn,
SVM = svm), resamples = "final")
resamps
summary(resamps)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl)
install.packages("RWeka")
library(RWeka)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "LMT",
trControl = fitControl)
install.packages("rpart.plot")
library(rpart.plot)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
parms = list(split = "information"),
trControl = fitControl)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
parms = list(split = "information"),
trControl = fitControl,
tuneLength = 10)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
parms = list(split = "information"),
trControl = fitControl,
tuneLength = 10)
library(rpart)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl,
metric = metric_used,
tuneLength = 10,
preProc = preProcessInTrain)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl,
metric = metric_used,
tuneLength = 10)
library(rpart)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl,
tuneLength = 10)
summary(resamps)
View(nb)
install.packages("kknn")
library(kknn)
set.seed(825)
knn <- train(Class ~ ., data = data_training,
method = "kknn",
trControl = fitControl)
resamps <- resamples(list(NB = nb,
NN = nn,
KNN = knn,
SVM = svm))
resamps
summary(resamps)
View(knn)
knn[["resample"]][["Accuracy"]]
library(snn)
set.seed(825)
snn <- train(Class ~ ., data = data_training,
method = "snn",
trControl = fitControl)
#1. Carga los datos en R. Nombra las columnas para identificar major el tablero,
#como si se visitarán de izquierda a derecha y de arriba a abajo.
data <- read.table("tic-tac-toe.data.txt", header=FALSE, sep=",")
names(data) <- c("top-left-square", "top-middle-square", "top-right-square",
"middle-left-square", "middle-middle-square", "middle-right-square", "bottom-left-square",
"bottom-middle-square", "bottom-right-square", "Class")
#Comprueba si hay valores faltantes.
any(is.na(data))
#2. Lee la sección "data splitting" de la web de “caret”. A continuación parte los
#datos en 70% para entrenamiento y 30% de test manteniendo la proporción original de
#clases.
library(caret)
set.seed(998)
inTraining <- createDataPartition(data$Class, p=.7, list=FALSE)
data_training <- data[ inTraining,]
data_testing  <- data[-inTraining,]
#3.
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1)
# Naive Bayes
library(naivebayes)
set.seed(825)
nb <- train(Class ~ ., data = data_training,
method = "naive_bayes",
trControl = fitControl)
# Decision Tree
library(rpart)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl,
tuneLength = 10)
# Neural Network
library(nnet)
set.seed(825)
nn <- train(Class ~ ., data = data_training,
method = "nnet",
trControl = fitControl)
# Nearest Neighbour
library(snn)
set.seed(825)
snn <- train(Class ~ ., data = data_training,
method = "snn",
trControl = fitControl)
# SVM (linear kernel)
library(kernlab)
set.seed(825)
svm <- train(Class ~ ., data = data_training,
method = "svmLinear",
trControl = fitControl)
resamps <- resamples(list(NB = nb,
NN = nn,
KNN = snn,
SVM = svm))
resamps
summary(resamps)
install.packages("rmarkdown")
---
title: "Untitled"
output: github_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## GitHub Documents
This is an R Markdown format used for publishing markdown documents to GitHub. When you click the **Knit** button all R code chunks are run and a markdown file (.md) suitable for publishing to GitHub is generated.
## Including Code
You can include R code in the document as follows:
```{r cars}
summary(cars)
```
## Including Plots
You can also embed plots, for example:
```{r pressure, echo=FALSE}
plot(pressure)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
render("document.Rmd")
library(rmarkdown)
render("1-example.Rmd")
library(rmarkdown)
render("document.Rmd")
#1. Carga los datos en R. Nombra las columnas para identificar major el tablero,
#como si se visitarán de izquierda a derecha y de arriba a abajo.
data <- read.table("tic-tac-toe.data.txt", header=FALSE, sep=",")
names(data) <- c("top-left-square", "top-middle-square", "top-right-square",
"middle-left-square", "middle-middle-square", "middle-right-square", "bottom-left-square",
"bottom-middle-square", "bottom-right-square", "Class")
#Comprueba si hay valores faltantes.
any(is.na(data))
#2. Lee la sección "data splitting" de la web de “caret”. A continuación parte los
#datos en 70% para entrenamiento y 30% de test manteniendo la proporción original de
#clases.
library(caret)
set.seed(998)
inTraining <- createDataPartition(data$Class, p=.7, list=FALSE)
data_training <- data[ inTraining,]
data_testing  <- data[-inTraining,]
#3.
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1)
# Naive Bayes
library(naivebayes)
set.seed(825)
nb <- train(Class ~ ., data = data_training,
method = "naive_bayes",
trControl = fitControl)
# Decision Tree
library(rpart)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl,
tuneLength = 10)
# Neural Network
library(nnet)
set.seed(825)
nn <- train(Class ~ ., data = data_training,
method = "nnet",
trControl = fitControl)
# Nearest Neighbour
library(snn)
set.seed(825)
snn <- train(Class ~ ., data = data_training,
method = "snn",
trControl = fitControl)
# SVM (linear kernel)
library(kernlab)
set.seed(825)
svm <- train(Class ~ ., data = data_training,
method = "svmLinear",
trControl = fitControl)
resamps <- resamples(list(NB = nb,
NN = nn,
KNN = snn,
SVM = svm))
resamps
summary(resamps)
setwd("~/masterAI/A3/Prácticas en R/Unidad 2")
knit_with_parameters('~/masterAI/A3/Prácticas en R/Unidad 2/document.Rmd')
#1. Carga los datos en R. Nombra las columnas para identificar major el tablero,
#como si se visitarán de izquierda a derecha y de arriba a abajo.
data <- read.table("tic-tac-toe.data.txt", header=FALSE, sep=",")
names(data) <- c("top-left-square", "top-middle-square", "top-right-square",
"middle-left-square", "middle-middle-square", "middle-right-square", "bottom-left-square",
"bottom-middle-square", "bottom-right-square", "Class")
#Comprueba si hay valores faltantes.
any(is.na(data))
library(caret)
set.seed(998)
inTraining <- createDataPartition(data$Class, p=.7, list=FALSE)
data_training <- data[ inTraining,]
data_testing  <- data[-inTraining,]
?kable
kable(row.names = c("Accuracy", "KAppa"))
kable(head(iris), format = "latex")
library(kable)
?table
table(c("Accuracy", "Kappa"))
table(c("Accuracy", "Kappa"), row.names("Naive Bayes"))
table(c("Accuracy", "Kappa"), row.names=c("Naive Bayes"))
#3.
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1)
#1. Carga los datos en R. Nombra las columnas para identificar major el tablero,
#como si se visitarán de izquierda a derecha y de arriba a abajo.
data <- read.table("tic-tac-toe.data.txt", header=FALSE, sep=",")
names(data) <- c("top-left-square", "top-middle-square", "top-right-square",
"middle-left-square", "middle-middle-square", "middle-right-square", "bottom-left-square",
"bottom-middle-square", "bottom-right-square", "Class")
#Comprueba si hay valores faltantes.
any(is.na(data))
#2. Lee la sección "data splitting" de la web de “caret”. A continuación parte los
#datos en 70% para entrenamiento y 30% de test manteniendo la proporción original de
#clases.
library(caret)
set.seed(998)
inTraining <- createDataPartition(data$Class, p=.7, list=FALSE)
data_training <- data[ inTraining,]
data_testing  <- data[-inTraining,]
#3.
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1)
# Naive Bayes
library(naivebayes)
set.seed(825)
nb <- train(Class ~ ., data = data_training,
method = "naive_bayes",
trControl = fitControl)
# Decision Tree
library(rpart)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl,
tuneLength = 10)
# Neural Network
library(nnet)
set.seed(825)
nn <- train(Class ~ ., data = data_training,
method = "nnet",
trControl = fitControl)
# Nearest Neighbour
library(snn)
set.seed(825)
snn <- train(Class ~ ., data = data_training,
method = "snn",
trControl = fitControl)
# SVM (linear kernel)
library(kernlab)
set.seed(825)
svm <- train(Class ~ ., data = data_training,
method = "svmLinear",
trControl = fitControl)
resamps <- resamples(list(NB = nb,
NN = nn,
KNN = snn,
SVM = svm))
resamps
summary(resamps)
#1. Carga los datos en R. Nombra las columnas para identificar major el tablero,
#como si se visitarán de izquierda a derecha y de arriba a abajo.
data <- read.table("tic-tac-toe.data.txt", header=FALSE, sep=",")
names(data) <- c("top-left-square", "top-middle-square", "top-right-square",
"middle-left-square", "middle-middle-square", "middle-right-square", "bottom-left-square",
"bottom-middle-square", "bottom-right-square", "Class")
#Comprueba si hay valores faltantes.
any(is.na(data))
#2. Lee la sección "data splitting" de la web de “caret”. A continuación parte los
#datos en 70% para entrenamiento y 30% de test manteniendo la proporción original de
#clases.
library(caret)
set.seed(998)
inTraining <- createDataPartition(data$Class, p=.7, list=FALSE)
data_training <- data[ inTraining,]
data_testing  <- data[-inTraining,]
#3.
fitControl <- trainControl(method = "cv",
number = 10,
repeats = 1)
# Naive Bayes
library(naivebayes)
set.seed(825)
nb <- train(Class ~ ., data = data_training,
method = "naive_bayes",
trControl = fitControl)
# Decision Tree
library(rpart)
set.seed(825)
dt <- train(Class ~ ., data = data_training,
method = "rpart",
trControl = fitControl,
tuneLength = 10)
# Neural Network
library(nnet)
set.seed(825)
nn <- train(Class ~ ., data = data_training,
method = "nnet",
trControl = fitControl)
# Nearest Neighbour
library(snn)
set.seed(825)
snn <- train(Class ~ ., data = data_training,
method = "snn",
trControl = fitControl)
# SVM (linear kernel)
library(kernlab)
set.seed(825)
svm <- train(Class ~ ., data = data_training,
method = "svmLinear",
trControl = fitControl)
resamps <- resamples(list(NB = nb,
NN = nn,
KNN = snn,
SVM = svm))
resamps
summary(resamps)
