Para finalizar, veremos una comparativa entre los algoritmos que hemos analizado en las secciones previas, tras ser aplicados sobre grafos aleatorios, también conocidos como grafos de Erdös y Renyi o grafos binomiales. Pero, en primer lugar, definimos el concepto de grafo aleatorio.

Se dice que un grafo es aleatorio si la presencia y colocación de sus aristas siguen una distribución aleatoria. Por tanto, un grafo aleatorio no puede diseñarse con ningún criterio concreto. La noción del modelo de un grafo aleatorio incluye el límite entre cada par de nodos con igual probabilidad, independientemente de los extremos. Concretamente del modelo se denomina: modelo de Erdös y Renyi. El nombre del modelo proviene de los matemáticos Paul Erdős y Alfréd Rényi\cite{ErdosRenyi}, quienes lo presentaron por primera en 1959.

El modelo Erdös y Renyi (a veces nombrado en la literatura abreviado como modelo ER), es el método que se ha empleado en la generación de los grafos aleatorios. Para ello, un grafo se construye mediante la conexión de los nodos al azar. Cada arista se incluye en el grafo con una probabilidad de \textit{p} independiente de las otras aristas del grafo. De manera equivalente, todos los grafos con \textit{n} nodos y \textit{m} aristas tienen la misma probabilidad de:

\begin{center}
	$p^m(1 - p)^{{n \choose 2} - m}$
\end{center}

Para la implementación de los grafos aleatorios se ha utilizado la librería de Python:  \href{https://networkx.github.io/documentation/latest/reference/generated/networkx.generators.random\_graphs.erdos\_renyi\_graph.html?highlight=erdos\%20renyi#networkx.generators.random\_graphs.erdos\_renyi\_graph}{NetworkX}. Concretamente el método que se ha utilizado nos devuelve un grafo no dirigido, donde se elige cada una de las aristas posibles con probabilidad \textit{p} = 0.7, según el número de nodos de entrada \textit{n}. En particular, el caso p = 0.7 se corresponde con el caso en el que todos los grafos en \textit{n} vértices se eligen con mayor probabilidad. Los pesos de las aristas también se han generado aleatoriamente dentro de un intervalo de pesos de 1 a 20.

Podemos observar la codificación utilizada a continuación, que se ejecuta en tiempo $O$($n^2$).

\lstset{language=Python}    
\begin{lstlisting}[frame=single]  
G = nx.erdos_renyi_graph(n, 0.7)

for u, v in G.edges():
if u != v:
G[u][v]['label'] = random.randrange(1, 20)
\end{lstlisting}

Finalmente, en la tabla siguiente, se presenta una comparación entre los algoritmos codificados sobre los grafos aleatorios de este trabajo en términos de tiempo computacional. Se muestra el tiempo (en segundos) en completar ejecuciones sobre conjuntos de grafos aleatorios para distintos números de vértices, \textit{n}. Para ello se ha utilizado un MacBook Pro, con un procesador de cuatro núcleos de 2.8 GHz y con 16 GB de memoria RAM. Es de esperar que, a mayor número de vértices, los algoritmos tengan mayor tiempo de ejecución.

\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textit{n} & 307 & 552 & 861 \\
		\hline
		Kernighan-Lin & 0.0117 & 0.0214 & 0.0451\\
		\hline
		Spectral Bisection & 0.0021 & 0.0031 & 0.0060 \\
		\hline
		Multilevel Spectral Bisection & 0.0025 & 0.0031 & 0.0037 \\ 
		\hline
	\end{tabular}
\end{center}

Podemos ver como los algoritmos Kernighan-Lin, Spectral Bisection y Multilevel Spectral Bisection tienen unos tiempos de ejecución similares cuando el número de vértices \textit{n} es más pequeño. Fenómeno que cambia cuando casi se duplican el número de vértices.

Si nos fijamos con el algoritmo Kernighan-Lin, cuando el número de vértices inicialmente casi se dobla, el tiempo de ejecución también lo hace. Parece que el algoritmo va aumentando el tiempo de ejecución a medida que aumenta el número de vértices de manera proporcionada.

En cambio, el algoritmo Spectral Bisection obtiene unos resultados muy parecidos con Multilevel Spectral Bisection, pero cuando el número de vértices empieza a ser muy elevado, la eficiencia baja drásticamente. Que no es el caso del algoritmo Multilevel Spectral Bisection.

Como conclusión global a la comparación entre los algoritmos podemos decir que el algoritmo Multilevel Spectral Bisection es el más eficiente para grafos no dirigidos que se ha codificado, y el algoritmo Kernighan-Lin es el menos eficiente.

En las secciones anteriores siempre hemos hablado de minimizar el tamaño de partición en ciertas condiciones. Pero para todos menos algunos gafos bien estructurados o pequeños, la minimización real no es factible porque tomaría mucho tiempo. Encontrar la solución óptima para el problema de partición de grafos es para todos los casos no triviales que se sabe que son NP-completos\cite{NPCompleteness}.

En pocas palabras, esto significa que no hay un algoritmo conocido que sea mucho más rápido que probar todas las combinaciones posibles y hay pocas esperanzas de que se encuentre uno. Un poco más exacto (pero aún ignorando algunos fundamentos de la teoría NP) significa que la bisección del grafo cae en una gran clase de problemas, todos los cuales pueden transformarse entre sí y para los cuales no hay algoritmos conocidos que puedan resolver el problema en tiempo polinómico en el tamaño de la entrada.

Entonces, en lugar de encontrar la solución óptima, recurrimos a la heurística. Es decir, tratamos de usar algoritmos que pueden no ofrecer la solución óptima cada vez, pero que darán una buena solución al menos la mayor parte del tiempo. Como veremos, a menudo hay una compensación entre el tiempo de ejecución y la calidad de la solución. Algunos algoritmos se ejecutan bastante rápido, pero solo encuentran una solución de calidad media, mientras que otros tardan mucho tiempo pero ofrecen soluciones excelentes e incluso otros se pueden ajustar entre ambos extremos.

La elección del tiempo frente a la calidad depende de la aplicación prevista. Para el diseño VLSI o el diseño de la red, puede ser aceptable esperar mucho tiempo porque una solución aún mejor puede ahorrar dinero real.

Por otro lado, en el contexto de la multiplicación escasa de matriz-vector, solo nos interesa el tiempo total. Por lo tanto, el tiempo de ejecución para la partición del gráfico debe ser menor que el tiempo ahorrado por la multiplicación de matriz-vector más rápida.
Si solo utilizamos una determinada matriz una vez, un algoritmo rápido que entrega solo particiones de calidad media en general podría ser más rápido que un algoritmo más lento con particiones de mejor calidad. Pero si usamos la misma matriz (o diferentes matrices con el mismo gráfico) a menudo, el algoritmo más lento podría ser preferible.

De hecho, hay incluso más factores a considerar. Hasta ahora queríamos que las diferentes particiones tuvieran el mismo peso. Como veremos en la sección 2.2, podría ser una ventaja aceptar particiones de un tamaño ligeramente diferente para lograr un mejor

tamaño de corte
Todo esto debería demostrar que no existe un mejor algoritmo único para todas las situaciones y que los diferentes algoritmos descritos en los siguientes capítulos tienen sus aplicaciones.


Después de crear las particiones, se calcula la distribución de nodos y bordes dentro de las particiones y entre las particiones. Esto ofrece una idea de cómo las proporciones de partición se desvían entre sí. Estas particiones se utilizan en dos algoritmos de validación diferentes, Pagerank y Breadth First Search, para ver la diferencia en la comunicación entre las particiones.
Este proyecto se centrará en particionar usando los valores de partición en los nodos del gráfico completo.
Los algoritmos de partición y los algoritmos distributivos se realizarán inicialmente para gráficos más pequeños. Luego, se utilizarán tipos de gráficos más grandes y diferentes con diferentes atributos para probar si el código
sigue produciendo resultados precisos.

además la eficiencia cambia con el número de particiones.

Los algoritmos de partición probablemente producirán resultados diferentes dependiendo de los atributos del gráfico de entrada.
